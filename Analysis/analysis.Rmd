---
title: "analysis"
author: "Nicholas Lester"
output:  
  github_document:
  pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, cache = T, cache.lazy = F, warning = F, message = F, error = F)
```

# Variation set analysis (second revision, Cognition)  
This file contains the code for the analysis of proportion of utterances belonging to variation sets in the ACQDIV corpora (child-surrounding speech; **CSS**), as well as two corpora of adult-directed speech (**ADS**).  
  
The analyses are divided into three broad sections:  
* Longitudinal analysis of VS proportions in ACQDIV  
* Cross-sectional analysis of VS proportions between ACQDIV and ADS  
* Comparisons of each of the prior analyses against random baselines  
  
## 1.0 Preliminaries
### 1.1 Clear memory
```{r clear_memory}
rm(list = ls(all = T))
```

### 1.2 Install/load libraries
Check if necessary libraries are installed. If so, load. If not, install and load.
```{r libraries}
list.of.packages =  c("lmerTest", 
                      "ggplot2", 
                      "visreg", 
                      "mgcv", 
                      "dplyr", 
                      "ggpubr", 
                      "car", 
                      "gridExtra", 
                      "cowplot")
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)){
    install.packages(new.packages)
}

invisible(lapply(list.of.packages, library, character.only = TRUE))
```

### 1.3 Load the data
The data are split into two sets: CSS-only (the primary analyses) and ADS-vs-CSS (supplementary comparisons of variation sets in English and Chintang ADS vs. CSS).

#### 1.3.1 All data
```{r load_data}
# Original data
load("../Data/Public/modeling_data.Rdata")

# Random data
load("../Data/Public/modeling_data_random.Rdata")
```

#### 1.3.2 Separate fuzzy/strict data for each dataset  
Here we create individual datasets for the type of match (SequenceMatcher, or *fuzzy*, and identity matching, or *strict*). We do this for both the original (i.e., true, or non-simulated data) and a randomized version of the same.  
  
##### 1.3.2.1 Original data
**CSS**
```{r separate_fuzzy_strict_original_css}
css = final.dataset %>%
      filter(!corpus %in% c("Chintang_Adults", "English_Adults")  &
             is.finite(logit.vs.percentage))

css.morph = css %>% 
            filter(level.of.analysis == "morpheme")

css.word = css %>%
           filter(level.of.analysis == "word")

#### Morphemes
morph.langs = c("Chintang", "Inuktitut", "Turkish", "Yucatec", "Sesotho")

css.morph.fuzzy = css.morph %>% filter(match_type == "fuzzy" & corpus %in% morph.langs)

css.morph.strict = css.morph %>% filter(match_type == "strict" & corpus %in% morph.langs)

#### Words
css.word.fuzzy = css.word %>% filter(match_type == "fuzzy")

css.word.strict = css.word %>% filter(match_type == "strict")
```
  
**ADS**
```{r separate_fuzzy_strict_original_ads}
## ADS
#### English
eng = final.dataset %>%
      filter(corpus %in% c("English_Adults", 
                           "English_Manchester1"))
###### Fuzzy
ads.fuzzy.eng = eng %>%
                filter(match_type == "fuzzy" &
                       level.of.analysis == "word")

###### Strict
ads.strict.eng = eng %>%
                 filter(match_type == "strict" &
                        level.of.analysis == "word")
#### Chintang
cnt = final.dataset %>%
      filter(corpus %in% c("Chintang_Adults", 
                           "Chintang"))
###### Fuzzy
ads.fuzzy.cnt = cnt %>%
                filter(match_type == "fuzzy" &
                       level.of.analysis == "word")

###### Strict
ads.strict.cnt = cnt %>%
                 filter(match_type == "strict" &
                        level.of.analysis == "word")
```

##### 1.3.2.2 Random data
**CSS**
```{r separate_fuzzy_strict_random_css}
css.rand = all.data.rand %>%
           filter(!corpus %in% c("English_Adults",
                                 "Chintang_Adults"))

#### Words
###### Fuzzy
css.word.fuzzy.rand = css.rand %>%
                      filter(match_type == "fuzzy" &
                             level.of.analysis == "word")
  
###### Strict
css.word.strict.rand = css.rand %>%
                       filter(match_type == "strict" &
                              level.of.analysis == "word")
#### Morphemes
###### Fuzzy
css.morph.fuzzy.rand = css.rand %>%
                      filter(match_type == "fuzzy" &
                             level.of.analysis == "morpheme")
###### Strict
css.morph.strict.rand = css.rand %>%
                        filter(match_type == "strict" &
                               level.of.analysis == "morpheme")
```
  
**ADS**
```{r separate_fuzzy_strict_random_ads}
#### English
eng.rand = all.data.rand %>%
           filter(corpus %in% c("English_Manchester1", 
                                "English_Adults"))
  
###### Fuzzy
ads.fuzzy.eng.rand = eng.rand %>%
                     filter(match_type == "fuzzy" &
                            level.of.analysis == "word")
                        
###### Strict
ads.strict.eng.rand = eng.rand %>%
                      filter(match_type == "strict" &
                             level.of.analysis == "word")
#### Chintang
cnt.rand = all.data.rand %>%
           filter(corpus %in% c("Chintang",
                                "Chintang_Adults"))
###### Fuzzy
ads.fuzzy.cnt.rand = cnt.rand %>%
                     filter(match_type == "fuzzy" &
                            level.of.analysis == "word")

###### Strict
ads.strict.cnt.rand = cnt.rand %>%
                      filter(match_type == "strict" &
                             level.of.analysis == "word")
```

### 1.4 Functions
The first two functions are necessary given that we are modeling proportions using linear models (we need to stretch the {0,1} range into {-inf,inf} range). The last deals with exporting model results efficiently (not implemented in the repository, but available to whoever is interested).
```{r transform_functions}
logit = function(p){
  return(log(p/(1-p)))
}

ilogit = function(x){
  return(exp(x)/(1+exp(x)))
}

# mod = the lmer model object of choice
# f = the location you would like to save the output
mod.sums = function(mod, f){
    mod.sum = summary(mod)
    write.table(file = f, round(mod.sum$coefficients, 3), sep="\t", quote=F, row.names=T)
}
```

## 2.0 Longitudinal analysis of CSS (no random baseline)
This section contains models designed to compare longitudinal effects across languages. It does not contain comparisons against random baselines. Different methods for removing outliers were applied. In the paper, we report the SD-based approach as it produced the best-behaved residuals.  
  
### 2.1 Modeling 
#### 2.1.1 Full models (corpus as interacting term)
In these models, we include all data points from all corpora. Language is treated as an interaction term with the relevant developmental index. Treatment contrasts are applied; visualization and/or reordering of levels prior to computing the model can reveal specific contrasts between languages. 

Models were fit with intercept adjustments for session, nested in speaker, nested in corpus. In some cases, we were forced to rely on the simpler speaker-nested-in-corpus structure (otherwise, the models break down). This difference does not seem to play any important role in the model outcomes.  

We use four indices of development to represent the "longitudinal" aspect of the analyses: age, MLU, lexical/morphological diversity, and the scores of a principal component analysis (PCA) from the principal component (which for all languages captured two or more of the developmental indices simultaneously).  
  
We also run models with and without outliers, where outliers are defined in two ways: values falling beyond (a) 1.5 times the interquartile range (IQR) above or below the mean and (b) 2 times the standard deviation above or below the mean.  

##### 2.1.1.1 Words, fuzzy
Word-level matching, with SequenceMatcher similarity criterion set to .55. Sub-sections correspond to each developmental index (here and for the following sections).  
  
**Age**
```{r full_model_words_fuzzy_age}
## Full sample
mod.words.fuzzy.age.full = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.fuzzy, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.age.full)
summary(mod.words.fuzzy.age.full)
qqnorm(resid(mod.words.fuzzy.age.full))
mod.sums(mod.words.fuzzy.age.full, "../testmod.txt")

## IQR outliers removed (simplified RE structure)
mod.words.fuzzy.age.iqr = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.fuzzy %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.age.iqr)
summary(mod.words.fuzzy.age.iqr)
qqnorm(resid(mod.words.fuzzy.age.iqr))

## 2*SD outliers removed
mod.words.fuzzy.age.sd = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.fuzzy  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.age.sd)
summary(mod.words.fuzzy.age.sd)
qqnorm(resid(mod.words.fuzzy.age.sd)) # this produces best looking residuals
```
  
**MLU**
```{r full_model_words_fuzzy_mlu}
## Full sample
mod.words.fuzzy.mlu.full = lmer(logit.vs.percentage ~ corpus*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.fuzzy, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.mlu.full)
summary(mod.words.fuzzy.mlu.full)
qqnorm(resid(mod.words.fuzzy.mlu.full))

## IQR outliers removed (simplified)
mod.words.fuzzy.mlu.iqr = lmer(logit.vs.percentage ~ corpus*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.fuzzy %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.mlu.iqr)
summary(mod.words.fuzzy.mlu.iqr)
qqnorm(resid(mod.words.fuzzy.mlu.iqr))

## 2*SD outliers removed
mod.words.fuzzy.mlu.sd = lmer(logit.vs.percentage ~ corpus*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.fuzzy  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.mlu.sd)
summary(mod.words.fuzzy.mlu.sd)
qqnorm(resid(mod.words.fuzzy.mlu.sd))
```
  
**PCA**
```{r full_model_words_fuzzy_pca}
# Using PCA
## Full sample (simplified RE structure)
mod.words.fuzzy.pca.full = lmer(logit.vs.percentage ~ corpus*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.fuzzy, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.pca.full)
summary(mod.words.fuzzy.pca.full)
qqnorm(resid(mod.words.fuzzy.pca.full))

## IQR outliers removed (simplified RE structure)
mod.words.fuzzy.pca.iqr = lmer(logit.vs.percentage ~ corpus*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.fuzzy %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.pca.iqr)
summary(mod.words.fuzzy.pca.iqr)
qqnorm(resid(mod.words.fuzzy.pca.iqr))

## 2*SD outliers removed
mod.words.fuzzy.pca.sd = lmer(logit.vs.percentage ~ corpus*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.fuzzy  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.pca.sd)
summary(mod.words.fuzzy.pca.sd)
qqnorm(resid(mod.words.fuzzy.pca.sd))
```
  
**Entropy**
```{r full_model_words_fuzzy_H}
## Full sample (simplified RE structure)
mod.words.fuzzy.ent.full = lmer(logit.vs.percentage ~ corpus*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.fuzzy, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.ent.full)
summary(mod.words.fuzzy.ent.full)
qqnorm(resid(mod.words.fuzzy.ent.full))

## IQR outliers removed
mod.words.fuzzy.ent.iqr = lmer(logit.vs.percentage ~ corpus*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.fuzzy %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.ent.iqr)
summary(mod.words.fuzzy.ent.iqr)
qqnorm(resid(mod.words.fuzzy.ent.iqr))

## 2*SD outliers removed
mod.words.fuzzy.ent.sd = lmer(logit.vs.percentage ~ corpus*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.fuzzy  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.fuzzy.ent.sd)
summary(mod.words.fuzzy.ent.sd)
qqnorm(resid(mod.words.fuzzy.ent.sd))

```
  
The residual plots demonstrate that (a) leaving in all datapoints performs just as badly as removing outliers defined as 1.5\*IQR above/below the mean and (b) removing outliers defined as 2\*SD above/below the mean improves the situation.
  
##### 2.1.1.2 Words, strict
**Age**
```{r full_model_words_strict_age}
## Full sample
mod.words.strict.age.full = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.strict, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.age.full)
summary(mod.words.strict.age.full)
qqnorm(resid(mod.words.strict.age.full))

## IQR outliers removed
mod.words.strict.age.iqr = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.strict %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.age.iqr)
summary(mod.words.strict.age.iqr)
qqnorm(resid(mod.words.strict.age.iqr))

## 2*SD outliers removed
mod.words.strict.age.sd = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.strict  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.age.sd)
summary(mod.words.strict.age.sd)
qqnorm(resid(mod.words.strict.age.sd)) # this produces best looking residuals
```
  
**MLU**
```{r full_model_words_strict_mlu}
# Using MLU
## Full sample (simplified RE structure)
mod.words.strict.mlu.full = lmer(logit.vs.percentage ~ corpus*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.strict, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.mlu.full)
summary(mod.words.strict.mlu.full)
qqnorm(resid(mod.words.strict.mlu.full))

## IQR outliers removed (simplified RE structure)
mod.words.strict.mlu.iqr = lmer(logit.vs.percentage ~ corpus*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.strict %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.mlu.iqr)
summary(mod.words.strict.mlu.iqr)
qqnorm(resid(mod.words.strict.mlu.iqr))

## 2*SD outliers removed
mod.words.strict.mlu.sd = lmer(logit.vs.percentage ~ corpus*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.strict  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.mlu.sd)
summary(mod.words.strict.mlu.sd)
qqnorm(resid(mod.words.strict.mlu.sd))
```
  
**PCA**
```{r full_model_words_strict_pca}
## Full sample
mod.words.strict.pca.full = lmer(logit.vs.percentage ~ corpus*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.strict, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.pca.full)
summary(mod.words.strict.pca.full)
qqnorm(resid(mod.words.strict.pca.full))

## IQR outliers removed (simplified RE structure)
mod.words.strict.pca.iqr = lmer(logit.vs.percentage ~ corpus*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.strict %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.pca.iqr)
summary(mod.words.strict.pca.iqr)
qqnorm(resid(mod.words.strict.pca.iqr))

## 2*SD outliers removed (simplified RE structure)
mod.words.strict.pca.sd = lmer(logit.vs.percentage ~ corpus*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.strict  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.pca.sd)
summary(mod.words.strict.pca.sd)
qqnorm(resid(mod.words.strict.pca.sd))
```
  
**Entropy**
```{r full_model_words_strict_H}
## Full sample (simplified RE structure)
mod.words.strict.ent.full = lmer(logit.vs.percentage ~ corpus*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.word.strict, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.ent.full)
summary(mod.words.strict.ent.full)
qqnorm(resid(mod.words.strict.ent.full))

## IQR outliers removed
mod.words.strict.ent.iqr = lmer(logit.vs.percentage ~ corpus*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.strict %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.ent.iqr)
summary(mod.words.strict.ent.iqr)
qqnorm(resid(mod.words.strict.ent.iqr))

## 2*SD outliers removed
mod.words.strict.ent.sd = lmer(logit.vs.percentage ~ corpus*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.word.strict  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.words.strict.ent.sd)
summary(mod.words.strict.ent.sd)
qqnorm(resid(mod.words.strict.ent.sd))

```
  
Again, the 2\*SD approach to outliers leads to the best performing model.  
  
##### 2.1.1.3 Morphemes, fuzzy
**Age**
```{r full_model_morph_fuzzy_age}
## Full sample (simplified RE structure; very simplified!)
mod.morph.fuzzy.age.full = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus), data = css.morph.fuzzy, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.age.full)
summary(mod.morph.fuzzy.age.full)
qqnorm(resid(mod.morph.fuzzy.age.full))

## IQR outliers removed
mod.morph.fuzzy.age.iqr = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.age.iqr)
summary(mod.morph.fuzzy.age.iqr)
qqnorm(resid(mod.morph.fuzzy.age.iqr))

## 2*SD outliers removed
mod.morph.fuzzy.age.sd = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.age.sd)
summary(mod.morph.fuzzy.age.sd)
qqnorm(resid(mod.morph.fuzzy.age.sd)) # this produces best looking residuals
```
  
**MLU**
```{r full_model_morph_fuzzy_mlu}
## Full sample (simplified RE structure)
mod.morph.fuzzy.mlu.full = lmer(logit.vs.percentage ~ corpus*MLUm.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.morph.fuzzy, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.mlu.full)
summary(mod.morph.fuzzy.mlu.full)
qqnorm(resid(mod.morph.fuzzy.mlu.full))

## IQR outliers removed
mod.morph.fuzzy.mlu.iqr = lmer(logit.vs.percentage ~ corpus*MLUm.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.mlu.iqr)
summary(mod.morph.fuzzy.mlu.iqr)
qqnorm(resid(mod.morph.fuzzy.mlu.iqr))

## 2*SD outliers removed
mod.morph.fuzzy.mlu.sd = lmer(logit.vs.percentage ~ corpus*MLUm.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.mlu.sd)
summary(mod.morph.fuzzy.mlu.sd)
qqnorm(resid(mod.morph.fuzzy.mlu.sd))
```
  
**PCA**
```{r full_model_morph_fuzzy_pca}
## Full sample
mod.morph.fuzzy.pca.full = lmer(logit.vs.percentage ~ corpus*PC1_morpheme + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.pca.full)
summary(mod.morph.fuzzy.pca.full)
qqnorm(resid(mod.morph.fuzzy.pca.full))

## IQR outliers removed
mod.morph.fuzzy.pca.iqr = lmer(logit.vs.percentage ~ corpus*PC1_morpheme + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.pca.iqr)
summary(mod.morph.fuzzy.pca.iqr)
qqnorm(resid(mod.morph.fuzzy.pca.iqr))

## 2*SD outliers removed
mod.morph.fuzzy.pca.sd = lmer(logit.vs.percentage ~ corpus*PC1_morpheme + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.pca.sd)
summary(mod.morph.fuzzy.pca.sd)
qqnorm(resid(mod.morph.fuzzy.pca.sd))
```
  
**Entropy**
```{r full_model_morph_fuzzy_H}
## Full sample (simplified RE structure)
mod.morph.fuzzy.ent.full = lmer(logit.vs.percentage ~ corpus*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.morph.fuzzy, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.ent.full)
summary(mod.morph.fuzzy.ent.full)
qqnorm(resid(mod.morph.fuzzy.ent.full))

## IQR outliers removed
mod.morph.fuzzy.ent.iqr = lmer(logit.vs.percentage ~ corpus*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.ent.iqr)
summary(mod.morph.fuzzy.ent.iqr)
qqnorm(resid(mod.morph.fuzzy.ent.iqr))

## 2*SD outliers removed
mod.morph.fuzzy.ent.sd = lmer(logit.vs.percentage ~ corpus*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.fuzzy  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.fuzzy.ent.sd)
summary(mod.morph.fuzzy.ent.sd)
qqnorm(resid(mod.morph.fuzzy.ent.sd))
```

##### 2.1.1.4 Morphemes, strict
**Age**
```{r full_model_morph_strict_age}
# Using age
## Full sample (simplified RE structure)
mod.morph.strict.age.full = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.morph.strict, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.age.full)
summary(mod.morph.strict.age.full)
qqnorm(resid(mod.morph.strict.age.full))

## IQR outliers removed (simplified RE structure)
mod.morph.strict.age.iqr = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.morph.strict %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.age.iqr)
summary(mod.morph.strict.age.iqr)
qqnorm(resid(mod.morph.strict.age.iqr))

## 2*SD outliers removed
mod.morph.strict.age.sd = lmer(logit.vs.percentage ~ corpus*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.strict  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.age.sd)
summary(mod.morph.strict.age.sd)
qqnorm(resid(mod.morph.strict.age.sd)) # this produces best looking residuals
```
  
**MLU**
```{r full_model_morph_strict_mlu}
## Full sample
mod.morph.strict.mlu.full = lmer(logit.vs.percentage ~ corpus*MLUm.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.strict, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.mlu.full)
summary(mod.morph.strict.mlu.full)
qqnorm(resid(mod.morph.strict.mlu.full))

## IQR outliers removed (simplified RE structure)
mod.morph.strict.mlu.iqr = lmer(logit.vs.percentage ~ corpus*MLUm.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.morph.strict %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.mlu.iqr)
summary(mod.morph.strict.mlu.iqr)
qqnorm(resid(mod.morph.strict.mlu.iqr))

## 2*SD outliers removed
mod.morph.strict.mlu.sd = lmer(logit.vs.percentage ~ corpus*MLUm.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.strict  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.mlu.sd)
summary(mod.morph.strict.mlu.sd)
qqnorm(resid(mod.morph.strict.mlu.sd))
```
  
**PCA**
```{r full_model_morph_strict_pca}
## Full sample
mod.morph.strict.pca.full = lmer(logit.vs.percentage ~ corpus*PC1_morpheme + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.strict, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.pca.full)
summary(mod.morph.strict.pca.full)
qqnorm(resid(mod.morph.strict.pca.full))

## IQR outliers removed (simplified RE structure)
mod.morph.strict.pca.iqr = lmer(logit.vs.percentage ~ corpus*PC1_morpheme + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.morph.strict %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.pca.iqr)
summary(mod.morph.strict.pca.iqr)
qqnorm(resid(mod.morph.strict.pca.iqr))

## 2*SD outliers removed
mod.morph.strict.pca.sd = lmer(logit.vs.percentage ~ corpus*PC1_morpheme + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.strict  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.pca.sd)
summary(mod.morph.strict.pca.sd)
qqnorm(resid(mod.morph.strict.pca.sd))
```
  
**Entropy**
```{r full_model_morph_strict_H}
## Full sample
mod.morph.strict.ent.full = lmer(logit.vs.percentage ~ corpus*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.strict, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.ent.full)
summary(mod.morph.strict.ent.full)
qqnorm(resid(mod.morph.strict.ent.full))

## IQR outliers removed
mod.morph.strict.ent.iqr = lmer(logit.vs.percentage ~ corpus*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child/session_id), data = css.morph.strict %>% filter(IQR.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.ent.iqr)
summary(mod.morph.strict.ent.iqr)
qqnorm(resid(mod.morph.strict.ent.iqr))

## 2*SD outliers removed (simplified RE structure)
mod.morph.strict.ent.sd = lmer(logit.vs.percentage ~ corpus*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|corpus/target.child), data = css.morph.strict  %>% filter(SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.morph.strict.ent.sd)
summary(mod.morph.strict.ent.sd)
qqnorm(resid(mod.morph.strict.ent.sd))
```

#### 2.1.2 Language-specific modeling
The following models mirror those above (2.1.1), but are performed on one language at a time. To save space, we compress the sections per language across both level of analysis (word vs. morpheme) and developmental index. 
  
##### 2.1.2.1 Chintang
**Age**
```{r full_models_chintang_age}
## Words
#### Strict
cnt.mod.w.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.w.s.a)
anova(cnt.mod.w.s.a)

#### Fuzzy
cnt.mod.w.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.w.f.a)
anova(cnt.mod.w.f.a)

## Morphemes
#### Strict
cnt.mod.m.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.m.s.a)
anova(cnt.mod.m.s.a)

#### Fuzzy
cnt.mod.m.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.m.f.a)
anova(cnt.mod.m.f.a)
```
  
**PCA**
```{r full_models_chintang_pca}
## Words
#### Strict
cnt.mod.w.s.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.w.s.p)
anova(cnt.mod.w.s.p)

#### Fuzzy
cnt.mod.w.f.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.w.f.p)
anova(cnt.mod.w.f.p)

## Morphemes
#### Strict
cnt.mod.m.s.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.m.s.p)
anova(cnt.mod.m.s.p)

#### Fuzzy
cnt.mod.m.f.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.m.f.p)
anova(cnt.mod.m.f.p)
```
  
**MLU**
```{r full_models_chintang_mlu}
## Words
#### Strict
cnt.mod.w.s.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.w.s.m)
anova(cnt.mod.w.s.m)

#### Fuzzy
cnt.mod.w.f.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.w.f.m)
anova(cnt.mod.w.f.m)

## Morphemes
#### Strict
cnt.mod.m.s.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.m.s.m)
anova(cnt.mod.m.s.m)

#### Fuzzy
cnt.mod.m.f.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.m.f.m)
anova(cnt.mod.m.f.m)
```
  
**Entropy**
```{r full_models_chintang_H}
# Using entropy
## Words
#### Strict
cnt.mod.w.s.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.w.s.h)
anova(cnt.mod.w.s.h)

#### Fuzzy
cnt.mod.w.f.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.w.f.h)
anova(cnt.mod.w.f.h)

## Morphemes
#### Strict
cnt.mod.m.s.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.m.s.h)
anova(cnt.mod.m.s.h)

#### Fuzzy
cnt.mod.m.f.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(cnt.mod.m.f.h)
anova(cnt.mod.m.f.h)
```

##### 2.1.2.2 English
**Age**
```{r full_models_english_age}
## Words
#### Strict
eng.mod.w.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(eng.mod.w.s.a)
anova(eng.mod.w.s.a)

#### Fuzzy
eng.mod.w.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(eng.mod.w.f.a)
anova(eng.mod.w.f.a)
```
  
**PCA**
```{r full_models_english_pca}
## Words
#### Strict
eng.mod.w.s.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(eng.mod.w.s.p)
anova(eng.mod.w.s.p)

#### Fuzzy
eng.mod.w.f.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(eng.mod.w.f.p)
anova(eng.mod.w.f.p)
```
  
**MLU**
```{r full_models_english_mlu}
## Words
#### Strict
eng.mod.w.s.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(eng.mod.w.s.m)
anova(eng.mod.w.s.m)

#### Fuzzy
eng.mod.w.f.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(eng.mod.w.f.m)
anova(eng.mod.w.f.m)
```
  
**Entropy**  
```{r full_models_english_H}
## Words
#### Strict
eng.mod.w.s.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(eng.mod.w.s.h)
anova(eng.mod.w.s.h)

#### Fuzzy
eng.mod.w.f.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(eng.mod.w.f.h)
anova(eng.mod.w.f.h)
```

##### 2.1.2.3 Inuktitut
**Age**
```{r full_models_inuktitut_age}
## Words
#### Strict
ink.mod.w.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.w.s.a)
anova(ink.mod.w.s.a)

#### Fuzzy
ink.mod.w.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.w.f.a)
anova(ink.mod.w.f.a)

## Morphemes
#### Strict
ink.mod.m.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.m.s.a)
anova(ink.mod.m.s.a)

#### Fuzzy
ink.mod.m.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.m.f.a)
anova(ink.mod.m.f.a)
```
  
**PCA**
```{r full_models_inuktitut_pca}
## Words
#### Strict
ink.mod.w.s.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.w.s.p)
anova(ink.mod.w.s.p)

#### Fuzzy
ink.mod.w.f.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.w.f.p)
anova(ink.mod.w.f.p)

## Morphemes
#### Strict
ink.mod.m.s.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.m.s.p)
anova(ink.mod.m.s.p)

#### Fuzzy
ink.mod.m.f.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.m.f.p)
anova(ink.mod.m.f.p)
```
  
**MLU**
```{r full_models_inuktitut_MLU}
## Words
#### Strict
ink.mod.w.s.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.w.s.m)
anova(ink.mod.w.s.m)

#### Fuzzy
ink.mod.w.f.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.w.f.m)
anova(ink.mod.w.f.m)

## Morphemes
#### Strict
ink.mod.m.s.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.m.s.m)
anova(ink.mod.m.s.m)

#### Fuzzy
ink.mod.m.f.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.m.f.m)
anova(ink.mod.m.f.m)
```
   
**Entropy**
```{r full_models_inuktitut_H}
# Using entropy
## Words
#### Strict
ink.mod.w.s.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.w.s.h)
anova(ink.mod.w.s.h)

#### Fuzzy
ink.mod.w.f.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.w.f.h)
anova(ink.mod.w.f.h)

## Morphemes
#### Strict
ink.mod.m.s.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.m.s.h)
anova(ink.mod.m.s.h)

#### Fuzzy
ink.mod.m.f.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ink.mod.m.f.h)
anova(ink.mod.m.f.h)
```
  
##### 2.1.2.4 Japanese
**Age**
```{r full_models_japanese_age}
#### Strict
jpn.mod.w.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(jpn.mod.w.s.a)
anova(jpn.mod.w.s.a)

## Fuzzy
jpn.mod.w.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(jpn.mod.w.f.a)
anova(jpn.mod.w.f.a)
```
  
**PCA**
```{r full_models_japanese_pca}
#### Strict
jpn.mod.w.s.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(jpn.mod.w.s.p)
anova(jpn.mod.w.s.p)

## Fuzzy
jpn.mod.w.f.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(jpn.mod.w.f.p)
anova(jpn.mod.w.f.p)
```
  
**MLU**
```{r full_models_japanese_mlu}
#### Strict
jpn.mod.w.s.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(jpn.mod.w.s.m)
anova(jpn.mod.w.s.m)

## Fuzzy
jpn.mod.w.f.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(jpn.mod.w.f.m)
anova(jpn.mod.w.f.m)
```
  
**Entropy**
```{r full_models_japanese_H}
#### Strict
jpn.mod.w.s.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(jpn.mod.w.s.m)
anova(jpn.mod.w.s.m)

## Fuzzy
jpn.mod.w.f.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(jpn.mod.w.f.h)
anova(jpn.mod.w.f.h)
```
  
##### 2.1.2.5 Russian
**Age**
```{r full_models_russian_age}
#### Strict
rus.mod.w.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(rus.mod.w.s.a)
anova(rus.mod.w.s.a)

## Fuzzy
rus.mod.w.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(rus.mod.w.f.a)
anova(rus.mod.w.f.a)
```
  
**PCA**
```{r full_models_russian_pca}
#### Strict
rus.mod.w.s.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(rus.mod.w.s.p)
anova(rus.mod.w.s.p)

## Fuzzy
rus.mod.w.f.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(rus.mod.w.f.p)
anova(rus.mod.w.f.p)
```
  
**MLU**
```{r full_models_russian_mlu}
#### Strict
rus.mod.w.s.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(rus.mod.w.s.m)
anova(rus.mod.w.s.m)

## Fuzzy
rus.mod.w.f.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(rus.mod.w.f.m)
anova(rus.mod.w.f.m)
```
  
**Entropy**
```{r full_models_russian_H}
#### Strict
rus.mod.w.s.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(rus.mod.w.s.h)
anova(rus.mod.w.s.h)

## Fuzzy
rus.mod.w.f.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(rus.mod.w.f.h)
anova(rus.mod.w.f.h)
```
  
##### 2.1.2.6 Sesotho
**Age**
```{r full_models_sesotho_age}
## Words
#### Strict
ses.mod.w.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.w.s.a)
anova(ses.mod.w.s.a)

#### Fuzzy
ses.mod.w.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.w.f.a)
anova(ses.mod.w.f.a)

## Morphemes
#### Strict
ses.mod.m.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.m.s.a)
anova(ses.mod.m.s.a)

#### Fuzzy
ses.mod.m.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.m.f.a)
anova(ses.mod.m.f.a)
```
  
**PCA**
```{r full_models_sesotho_pca}
## Words
#### Strict
ses.mod.w.s.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.w.s.p)
anova(ses.mod.w.s.p)

#### Fuzzy
ses.mod.w.f.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.w.f.p)
anova(ses.mod.w.f.p)

## Morphemes
#### Strict
ses.mod.m.s.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.m.s.p)
anova(ses.mod.m.s.p)

#### Fuzzy
ses.mod.m.f.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.m.f.p)
anova(ses.mod.m.f.p)
```
  
**MLU**
```{r full_models_sesotho_mlu}
## Words
#### Strict
ses.mod.w.s.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.w.s.m)
anova(ses.mod.w.s.m)

#### Fuzzy
ses.mod.w.f.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.w.f.m)
anova(ses.mod.w.f.m)

## Morphemes
#### Strict
ses.mod.m.s.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.m.s.m)
anova(ses.mod.m.s.m)

#### Fuzzy
ses.mod.m.f.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.m.f.m)
anova(ses.mod.m.f.m)
```
  
**Entropy**
```{r full_models_sesotho_H}
## Words
#### Strict
ses.mod.w.s.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.w.s.h)
anova(ses.mod.w.s.h)

#### Fuzzy
ses.mod.w.f.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.w.f.h)
anova(ses.mod.w.f.h)

## Morphemes
#### Strict
ses.mod.m.s.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.m.s.h)
anova(ses.mod.m.s.h)

#### Fuzzy
ses.mod.m.f.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(ses.mod.m.f.h)
anova(ses.mod.m.f.h)
```
  
##### 2.1.2.7 Turkish
**Age**
```{r full_models_turkish_age}
## Words
#### Strict
tur.mod.w.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.w.s.a)
anova(tur.mod.w.s.a)

#### Fuzzy
tur.mod.w.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.w.f.a)
anova(tur.mod.w.f.a)

## Morphemes
#### Strict
tur.mod.m.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.m.s.a)
anova(tur.mod.m.s.a)

#### Fuzzy
tur.mod.m.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.m.f.a)
anova(tur.mod.m.f.a)
```
  
**PCA**
```{r full_models_turkish_pca}
## Words
#### Strict
tur.mod.w.s.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.w.s.p)
anova(tur.mod.w.s.p)

#### Fuzzy
tur.mod.w.f.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.w.f.p)
anova(tur.mod.w.f.p)

## Morphemes
#### Strict
tur.mod.m.s.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.m.s.p)
anova(tur.mod.m.s.p)

#### Fuzzy
tur.mod.m.f.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.m.f.p)
anova(tur.mod.m.f.p)
```
  
**MLU**
```{r full_models_turkish_mlu}
## Words
#### Strict
tur.mod.w.s.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.w.s.m)
anova(tur.mod.w.s.m)

#### Fuzzy
tur.mod.w.f.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.w.f.m)
anova(tur.mod.w.f.m)

## Morphemes
#### Strict
tur.mod.m.s.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.m.s.m)
anova(tur.mod.m.s.m)

#### Fuzzy
tur.mod.m.f.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.m.f.m)
anova(tur.mod.m.f.m)
```
  
**Entropy**
```{r full_models_turkish_H}
## Words
#### Strict
tur.mod.w.s.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.w.s.h)
anova(tur.mod.w.s.h)

#### Fuzzy
tur.mod.w.f.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.w.f.h)
anova(tur.mod.w.f.h)

## Morphemes
#### Strict
tur.mod.m.s.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.m.s.h)
anova(tur.mod.m.s.h)

#### Fuzzy
tur.mod.m.f.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(tur.mod.m.f.h)
anova(tur.mod.m.f.h)
```
  
##### 2.1.2.8 Yucatec
**Age**
```{r full_models_yucatec_age}
## Words
#### Strict
yuc.mod.w.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.w.s.a)
anova(yuc.mod.w.s.a)

#### Fuzzy
yuc.mod.w.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.w.f.a)
anova(yuc.mod.w.f.a)

## Morphemes
#### Strict
yuc.mod.m.s.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.m.s.a)
anova(yuc.mod.m.s.a)

#### Fuzzy
yuc.mod.m.f.a = lmer(logit.vs.percentage ~ logAge.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.m.f.a)
anova(yuc.mod.m.f.a)
```
  
**PCA**
```{r full_models_yucatec_pca}
## Words
#### Strict
yuc.mod.w.s.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.w.s.p)
anova(yuc.mod.w.s.p)

#### Fuzzy
yuc.mod.w.f.p = lmer(logit.vs.percentage ~ PC1_word + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.w.f.p)
anova(yuc.mod.w.f.p)

## Morphemes
#### Strict
yuc.mod.m.s.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.m.s.p)
anova(yuc.mod.m.s.p)

#### Fuzzy
yuc.mod.m.f.p = lmer(logit.vs.percentage ~ PC1_morpheme + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.m.f.p)
anova(yuc.mod.m.f.p)
```
  
**MLU**
```{r full_models_yucatec_mlu}
## Words
#### Strict
yuc.mod.w.s.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.w.s.m)
anova(yuc.mod.w.s.m)

#### Fuzzy
yuc.mod.w.f.m = lmer(logit.vs.percentage ~ MLUw.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.w.f.m)
anova(yuc.mod.w.f.m)

## Morphemes
#### Strict
yuc.mod.m.s.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.m.s.m)
anova(yuc.mod.m.s.m)

#### Fuzzy
yuc.mod.m.f.m = lmer(logit.vs.percentage ~ MLUm.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.m.f.m)
anova(yuc.mod.m.f.m)
```
  
**Entropy**
```{r full_models_yucatec_H}
## Words
#### Strict
yuc.mod.w.s.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.strict %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.w.s.h)
anova(yuc.mod.w.s.h)

#### Fuzzy
yuc.mod.w.f.h = lmer(logit.vs.percentage ~ WordEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.word.fuzzy %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.w.f.h)
anova(yuc.mod.w.f.h)

## Morphemes
#### Strict
yuc.mod.m.s.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.strict %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.m.s.h)
anova(yuc.mod.m.s.h)

#### Fuzzy
yuc.mod.m.f.h = lmer(logit.vs.percentage ~ MorphEnt.scaled + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|target.child/session_id), data = css.morph.fuzzy %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(yuc.mod.m.f.h)
anova(yuc.mod.m.f.h)
```
  
##### 2.1.3 Plotting
**Plots combining all of the languages**
```{r combo_plots}
facet.labs.words = c("Chintang", "English", "Inuktitut", "Japanese", "Russian", "Sesotho", "Turkish", "Yucatec")
names(facet.labs.words) = c("Chintang", "English_Manchester1", "Inuktitut", "Japanese_MiiPro", "Russian", "Sesotho", "Turkish", "Yucatec")

## Words
#### Age as developmental index
###### 2*SD-based outliers removed
######## Fuzzy
words.fuzzy.age.sd.plot = visreg(mod.words.fuzzy.age.sd, "logAge.scaled", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Words: Fuzzy (age, without SD outliers)") + ylim(0, .6) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = words.fuzzy.age.sd.plot$data[,2], x = words.fuzzy.age.sd.plot$data[,1]), method="lm", formula=y~x); words.fuzzy.age.sd.plot

###### Strict
words.strict.age.sd.plot = visreg(mod.words.strict.age.sd, "logAge.scaled", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Words: Strict (age, without SD outliers)") + ylim(0, .6) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = words.strict.age.sd.plot$data[,2], x = words.strict.age.sd.plot$data[,1]), method="lm", formula=y~x); words.strict.age.sd.plot

#### PCA as developmental index
###### 2*SD-based outliers removed
######## Fuzzy
words.fuzzy.pca.sd.plot = visreg(mod.words.fuzzy.pca.sd, "PC1_word", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Words: Fuzzy (PCA, without SD outliers)") + ylim(0, .6) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = words.fuzzy.pca.sd.plot$data[,2], x = words.fuzzy.pca.sd.plot$data[,1]), method="lm", formula=y~x); words.fuzzy.pca.sd.plot

###### Strict
words.strict.pca.sd.plot = visreg(mod.words.strict.pca.sd, "PC1_word", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Words: Strict (PCA, without SD outliers)") + ylim(0, .6) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = words.strict.pca.sd.plot$data[,2], x = words.strict.pca.sd.plot$data[,1]), method="lm", formula=y~x); words.strict.pca.sd.plot

#### MLU as developmental index
###### Fuzzy
words.fuzzy.mlu.sd.plot = visreg(mod.words.fuzzy.mlu.sd, "MLUw.scaled", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Words: Fuzzy (MLU, full sample)") + ylim(0, .5) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = words.fuzzy.mlu.sd.plot$data[,2], x = words.fuzzy.mlu.sd.plot$data[,1]), method="lm", formula=y~x); words.fuzzy.mlu.sd.plot

###### Strict
words.strict.mlu.sd.plot = visreg(mod.words.strict.mlu.sd, "MLUw.scaled", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Words: Strict (MLU, full sample)") + ylim(0, 1) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = words.strict.mlu.sd.plot$data[,2], x = words.strict.mlu.sd.plot$data[,1]), method="lm", formula=y~x); words.strict.mlu.sd.plot

## Morphemes
#### Age as developmental index
###### 2*SD-based outliers removed
######## Fuzzy
morph.fuzzy.age.sd.plot = visreg(mod.morph.fuzzy.age.sd, "logAge.scaled", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Morphemes: Fuzzy (age, without SD outliers)") + ylim(0, .6) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = morph.fuzzy.age.sd.plot$data[,2], x = morph.fuzzy.age.sd.plot$data[,1]), method="lm", formula=y~x); morph.fuzzy.age.sd.plot

###### Strict
morph.strict.age.sd.plot = visreg(mod.morph.strict.age.sd, "logAge.scaled", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Morphemes: Strict (age, without SD outliers)") + ylim(0, .6) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = morph.strict.age.sd.plot$data[,2], x = morph.strict.age.sd.plot$data[,1]), method="lm", formula=y~x); morph.strict.age.sd.plot

#### PCA as developmental index
###### 2*SD-based outliers removed
######## Fuzzy
morph.fuzzy.pca.sd.plot = visreg(mod.morph.fuzzy.pca.sd, "PC1_morpheme", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Morphemes: Fuzzy (PCA, without 2*SD outliers)") + ylim(0, .6) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = morph.fuzzy.pca.sd.plot$data[,2], x = morph.fuzzy.pca.sd.plot$data[,1]), method="lm", formula=y~x); morph.fuzzy.pca.sd.plot

###### Strict
morph.strict.pca.sd.plot = visreg(mod.morph.strict.pca.sd, "PC1_morpheme", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Morphemes: Strict (PCA, without 2*SD outliers)") + ylim(0, .6) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = morph.strict.pca.sd.plot$data[,2], x = morph.strict.pca.sd.plot$data[,1]), method="lm", formula=y~x); morph.strict.pca.sd.plot

#### MLU as developmental index
###### Fuzzy
morph.fuzzy.mlu.sd.plot = visreg(mod.morph.fuzzy.mlu.sd, "MLUm.scaled", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Morphemes: Fuzzy (MLU, without 2*SD outliers)") + ylim(0, .5) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = morph.fuzzy.mlu.sd.plot$data[,2], x = morph.fuzzy.mlu.sd.plot$data[,1]), method="lm", formula=y~x); morph.fuzzy.mlu.sd.plot

###### Strict
morph.strict.mlu.sd.plot = visreg(mod.morph.strict.mlu.sd, "MLUm.scaled", by="corpus", cond=list(window_size = 5), partial=T, band=T, trans=ilogit, line.par = list(lty=0), points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), gg=T) + theme_bw() + ggtitle("Morphemes: Strict (MLU, without 2*SD outliers)") + ylim(0, 1) + theme(text = element_text(size=12, family="Times New Roman", face="bold"), plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank()) + facet_wrap(~corpus, nrow=3, labeller = labeller(corpus = facet.labs.words)) + geom_smooth(aes(y = morph.strict.mlu.sd.plot$data[,2], x = morph.strict.mlu.sd.plot$data[,1]), method="lm", formula=y~x); morph.strict.mlu.sd.plot 

# Save images
## Words
ggsave(words.fuzzy.age.sd.plot, filename = "../Results/Plots/word_fuzzy_age_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(words.strict.age.sd.plot, filename = "../Results/Plots/word_strict_age_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(words.fuzzy.pca.sd.plot, filename = "../Results/Plots/word_fuzzy_pca_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(words.strict.pca.sd.plot, filename = "../Results/Plots/word_strict_pca_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(words.fuzzy.mlu.sd.plot, filename = "../Results/Plots/word_fuzzy_mlu_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(words.strict.mlu.sd.plot, filename = "../Results/Plots/word_strict_mlu_sd.png", dpi="print", units = "in", height = 5, width = 4)

## Morphemes
ggsave(morph.fuzzy.age.sd.plot, filename = "../Results/Plots/morph_fuzzy_age_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(morph.strict.age.sd.plot, filename = "../Results/Plots/morph_strict_age_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(morph.fuzzy.pca.sd.plot, filename = "../Results/Plots/morph_fuzzy_pca_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(morph.strict.pca.sd.plot, filename = "../Results/Plots/morph_strict_pca_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(morph.fuzzy.mlu.sd.plot, filename = "../Results/Plots/morph_fuzzy_mlu_sd.png", dpi="print", units = "in", height = 5, width = 4)

ggsave(morph.strict.mlu.sd.plot, filename = "../Results/Plots/morph_strict_mlu_sd.png", dpi="print", units = "in", height = 5, width = 4)
```
  
**By-window-size plots**
```{r by-window_plots}
window.plotter = function(mod.dat, lang, title, dev.index){
  plots = list()
  counter=1
  for(n in c(3, 6, 9)){
      p = visreg(mod.dat, dev.index, 
             by="corpus", 
             cond=list(window_size = n), 
             partial=T, 
             band=T, 
             trans=ilogit,
             line.par = list(lty=0), 
             points.par = list(pch=".", col=rgb(.8, .2, .2, .5)), 
             gg=T)
      
       plot.dat = p[[1]]
      
       p = ggplot(plot.dat[plot.dat$corpus == lang,], aes(x, y)) +
                  geom_point(alpha = .2, color="red", size=.5) + 
                  stat_smooth(method="lm", formula = y ~ x) + 
                  ggtitle(paste0("Window: ", n)) +
                  theme_bw() +
                  theme(plot.title = element_text(hjust = 0.5),
                        axis.title.x = element_blank(),
                        axis.title.y = element_blank(),
                        text = element_text(size=12, family="Times New Roman", face="bold")) +
            ylim(0, 1)
      
      plots[[counter]] = p
      counter = counter + 1
  }
  
  p.total = ggarrange(plots[[1]], plots[[2]], plots[[3]], ncol = 3)
  if(dev.index == "logAge.scaled"){
      bottom.label = "Age (days, scaled)"
  }
  else if(dev.index == "MLUm.scaled" | dev.index == "MLUw.scaled"){
      bottom.label = "MLU (scaled)"
  }
  else{
      bottom.label = "PCA score"
  }
  p.total = annotate_figure(p.total, top = text_grob(title, family="Times New Roman", face="bold"), left = text_grob("Proportion of variation sets", family="Times New Roman", rot=90, face="bold"), bottom = text_grob(bottom.label, family="Times New Roman", face="bold"))
  return(p.total)
}

## Words
#### Age
###### Fuzzy
window.plotter(mod.words.fuzzy.age.sd, "Chintang", "Chintang: Words, Fuzzy", "logAge.scaled")

window.plotter(mod.words.fuzzy.age.sd, "English_Manchester1", "English: Words, Fuzzy", "logAge.scaled")

window.plotter(mod.words.fuzzy.age.sd, "Inuktitut", "Inuktitut: Words, Fuzzy", "logAge.scaled")

window.plotter(mod.words.fuzzy.age.sd, "Japanese_MiiPro", "Japanese: Words, Fuzzy", "logAge.scaled")

window.plotter(mod.words.fuzzy.age.sd, "Russian", "Russian: Words, Fuzzy", "logAge.scaled")

window.plotter(mod.words.fuzzy.age.sd, "Sesotho", "Sesotho: Words, Fuzzy", "logAge.scaled")

tur.w.f.age = window.plotter(mod.words.fuzzy.age.sd, "Turkish", "Turkish: Words, Fuzzy", "logAge.scaled")

yuc.w.f.age = window.plotter(mod.words.fuzzy.age.sd, "Yucatec", "Yucatec: Words, Fuzzy", "logAge.scaled")

###### Strict
window.plotter(mod.words.strict.age.sd, "Chintang", "Chintang: Words, Strict", "logAge.scaled")

window.plotter(mod.words.strict.age.sd, "English_Manchester1", "English: Words, Strict", "logAge.scaled")

window.plotter(mod.words.strict.age.sd, "Inuktitut", "Inuktitut: Words, Strict", "logAge.scaled")

window.plotter(mod.words.strict.age.sd, "Japanese_MiiPro", "Japanese: Words, Strict", "logAge.scaled")

window.plotter(mod.words.strict.age.sd, "Russian", "Russian: Words, Strict", "logAge.scaled")

window.plotter(mod.words.strict.age.sd, "Sesotho", "Sesotho: Words, Strict", "logAge.scaled")

window.plotter(mod.words.strict.age.sd, "Turkish", "Turkish: Words, Strict", "logAge.scaled")

window.plotter(mod.words.strict.age.sd, "Yucatec", "Yucatec: Words, Strict", "logAge.scaled")

#### MLU
###### Fuzzy
window.plotter(mod.words.fuzzy.mlu.sd, "Chintang", "Chintang: Words, Fuzzy", "MLUw.scaled")

window.plotter(mod.words.fuzzy.mlu.sd, "English_Manchester1", "English: Words, Fuzzy", "MLUw.scaled")

window.plotter(mod.words.fuzzy.mlu.sd, "Inuktitut", "Inuktitut: Words, Fuzzy", "MLUw.scaled")

window.plotter(mod.words.fuzzy.mlu.sd, "Japanese_MiiPro", "Japanese: Words, Fuzzy", "MLUw.scaled")

window.plotter(mod.words.fuzzy.mlu.sd, "Russian", "Russian: Words, Fuzzy", "MLUw.scaled")

window.plotter(mod.words.fuzzy.mlu.sd, "Sesotho", "Sesotho: Words, Fuzzy", "MLUw.scaled")

tur.w.f.age = window.plotter(mod.words.fuzzy.mlu.sd, "Turkish", "Turkish: Words, Fuzzy", "MLUw.scaled")

yuc.w.f.age = window.plotter(mod.words.fuzzy.mlu.sd, "Yucatec", "Yucatec: Words, Fuzzy", "MLUw.scaled")

###### Strict
window.plotter(mod.words.strict.mlu.sd, "Chintang", "Chintang: Words, Strict", "MLUw.scaled")

window.plotter(mod.words.strict.mlu.sd, "English_Manchester1", "English: Words, Strict", "MLUw.scaled")

window.plotter(mod.words.strict.mlu.sd, "Inuktitut", "Inuktitut: Words, Strict", "MLUw.scaled")

window.plotter(mod.words.strict.mlu.sd, "Japanese_MiiPro", "Japanese: Words, Strict", "MLUw.scaled")

window.plotter(mod.words.strict.mlu.sd, "Russian", "Russian: Words, Strict", "MLUw.scaled")

window.plotter(mod.words.strict.mlu.sd, "Sesotho", "Sesotho: Words, Strict", "MLUw.scaled")

window.plotter(mod.words.strict.mlu.sd, "Turkish", "Turkish: Words, Strict", "MLUw.scaled")

window.plotter(mod.words.strict.mlu.sd, "Yucatec", "Yucatec: Words, Strict", "MLUw.scaled")

#### PCA
###### Fuzzy
window.plotter(mod.words.fuzzy.pca.sd, "Chintang", "Chintang: Words, Fuzzy", "PC1_word")

window.plotter(mod.words.fuzzy.pca.sd, "English_Manchester1", "English: Words, Fuzzy", "PC1_word")

window.plotter(mod.words.fuzzy.pca.sd, "Inuktitut", "Inuktitut: Words, Fuzzy", "PC1_word")

window.plotter(mod.words.fuzzy.pca.sd, "Japanese_MiiPro", "Japanese: Words, Fuzzy", "PC1_word")

window.plotter(mod.words.fuzzy.pca.sd, "Russian", "Russian: Words, Fuzzy", "PC1_word")

window.plotter(mod.words.fuzzy.pca.sd, "Sesotho", "Sesotho: Words, Fuzzy", "PC1_word")

tur.w.f.age = window.plotter(mod.words.fuzzy.pca.sd, "Turkish", "Turkish: Words, Fuzzy", "PC1_word")

yuc.w.f.age = window.plotter(mod.words.fuzzy.pca.sd, "Yucatec", "Yucatec: Words, Fuzzy", "PC1_word")

###### Strict
window.plotter(mod.words.strict.pca.sd, "Chintang", "Chintang: Words, Strict", "PC1_word")

window.plotter(mod.words.strict.pca.sd, "English_Manchester1", "English: Words, Strict", "PC1_word")

window.plotter(mod.words.strict.pca.sd, "Inuktitut", "Inuktitut: Words, Strict", "PC1_word")

window.plotter(mod.words.strict.pca.sd, "Japanese_MiiPro", "Japanese: Words, Strict", "PC1_word")

window.plotter(mod.words.strict.pca.sd, "Russian", "Russian: Words, Strict", "PC1_word")

window.plotter(mod.words.strict.pca.sd, "Sesotho", "Sesotho: Words, Strict", "PC1_word")

window.plotter(mod.words.strict.pca.sd, "Turkish", "Turkish: Words, Strict", "PC1_word")

window.plotter(mod.words.strict.pca.sd, "Yucatec", "Yucatec: Words, Strict", "PC1_word")

## Morphemes
#### Age
###### Fuzzy
window.plotter(mod.morph.fuzzy.age.sd, "Chintang", "Chintang: Morphemes, Fuzzy", "logAge.scaled")

window.plotter(mod.morph.fuzzy.age.sd, "Inuktitut", "Inuktitut: Morphemes, Fuzzy", "logAge.scaled")

window.plotter(mod.morph.fuzzy.age.sd, "Sesotho", "Sesotho: Morphemes, Fuzzy", "logAge.scaled")

tur.m.f = window.plotter(mod.morph.fuzzy.age.sd, "Turkish", "Turkish: Morphemes, Fuzzy", "logAge.scaled")

yuc.m.f = window.plotter(mod.morph.fuzzy.age.sd, "Yucatec", "Yucatec: Morphemes, Fuzzy", "logAge.scaled")

###### Strict
window.plotter(mod.morph.strict.age.sd, "Chintang", "Chintang: Morphemes, Strict", "logAge.scaled")

window.plotter(mod.morph.strict.age.sd, "Inuktitut", "Inuktitut: Morphemes, Strict", "logAge.scaled")

window.plotter(mod.morph.strict.age.sd, "Sesotho", "Sesotho: Morphemes, Strict", "logAge.scaled")

window.plotter(mod.morph.strict.age.sd, "Turkish", "Turkish: Morphemes, Strict", "logAge.scaled")

window.plotter(mod.morph.strict.age.sd, "Yucatec", "Yucatec: Morphemes, Strict", "logAge.scaled")

#### MLU
###### Fuzzy
window.plotter(mod.morph.fuzzy.mlu.sd, "Chintang", "Chintang: Morphemes, Fuzzy", "MLUm.scaled")

window.plotter(mod.morph.fuzzy.mlu.sd, "Inuktitut", "Inuktitut: Morphemes, Fuzzy", "MLUm.scaled")

window.plotter(mod.morph.fuzzy.mlu.sd, "Sesotho", "Sesotho: Morphemes, Fuzzy", "MLUm.scaled")

tur.m.f = window.plotter(mod.morph.fuzzy.mlu.sd, "Turkish", "Turkish: Morphemes, Fuzzy", "MLUm.scaled")

yuc.m.f = window.plotter(mod.morph.fuzzy.mlu.sd, "Yucatec", "Yucatec: Morphemes, Fuzzy", "MLUm.scaled")

###### Strict
window.plotter(mod.morph.strict.mlu.sd, "Chintang", "Chintang: Morphemes, Strict", "MLUm.scaled")

window.plotter(mod.morph.strict.mlu.sd, "Inuktitut", "Inuktitut: Morphemes, Strict", "MLUm.scaled")

window.plotter(mod.morph.strict.mlu.sd, "Sesotho", "Sesotho: Morphemes, Strict", "MLUm.scaled")

window.plotter(mod.morph.strict.mlu.sd, "Turkish", "Turkish: Morphemes, Strict", "MLUm.scaled")

window.plotter(mod.morph.strict.mlu.sd, "Yucatec", "Yucatec: Morphemes, Strict", "MLUm.scaled")

#### PCA
###### Fuzzy
window.plotter(mod.morph.fuzzy.pca.sd, "Chintang", "Chintang: Morphemes, Fuzzy", "PC1_morpheme")

window.plotter(mod.morph.fuzzy.pca.sd, "Inuktitut", "Inuktitut: Morphemes, Fuzzy", "PC1_morpheme")

window.plotter(mod.morph.fuzzy.pca.sd, "Sesotho", "Sesotho: Morphemes, Fuzzy", "PC1_morpheme")

tur.w.f.age = window.plotter(mod.morph.fuzzy.pca.sd, "Turkish", "Turkish: Morphemes, Fuzzy", "PC1_morpheme")

yuc.w.f.age = window.plotter(mod.morph.fuzzy.pca.sd, "Yucatec", "Yucatec: Morphemes, Fuzzy", "PC1_morpheme")

###### Strict
window.plotter(mod.morph.strict.pca.sd, "Chintang", "Chintang: Morphemes, Strict", "PC1_morpheme")

window.plotter(mod.morph.strict.pca.sd, "Inuktitut", "Inuktitut: Morphemes, Strict", "PC1_morpheme")

window.plotter(mod.morph.strict.pca.sd, "Sesotho", "Sesotho: Morphemes, Strict", "PC1_morpheme")

window.plotter(mod.morph.strict.pca.sd, "Turkish", "Turkish: Morphemes, Strict", "PC1_morpheme")

window.plotter(mod.morph.strict.pca.sd, "Yucatec", "Yucatec: Morphemes, Strict", "PC1_morpheme")

```

## 3.0 Cross-sectional analysis of ADS vs CSS (no random baseline)
For these models, we stick with eliminating the 2\*SD outliers, as they have led to the best model fits.  

### 3.1 English
**Age**
```{r ads_english_age}
#### Fuzzy
mod.ads.eng.fuzzy.age = lmer(logit.vs.percentage ~ age.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.eng %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.eng.fuzzy.age)
summary(mod.ads.eng.fuzzy.age)
qqnorm(resid(mod.ads.eng.fuzzy.age))

#### Strict
mod.ads.eng.strict.age = lmer(logit.vs.percentage ~ age.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.eng %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.eng.strict.age)
summary(mod.ads.eng.strict.age)
qqnorm(resid(mod.ads.eng.strict.age))
```

**MLU**
```{r ads_english_mlu}
#### Fuzzy
mod.ads.eng.fuzzy.mlu = lmer(logit.vs.percentage ~ MLUw.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.eng %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.eng.fuzzy.mlu)
summary(mod.ads.eng.fuzzy.mlu)
qqnorm(resid(mod.ads.eng.fuzzy.mlu))

#### Strict
mod.ads.eng.strict.mlu = lmer(logit.vs.percentage ~ MLUw.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.eng %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.eng.strict.mlu)
summary(mod.ads.eng.strict.mlu)
qqnorm(resid(mod.ads.eng.strict.mlu))
```

**PCA**
```{r ads_english_pca}
#### Fuzzy
mod.ads.eng.fuzzy.pca = lmer(logit.vs.percentage ~ PC.word.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.eng %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.eng.fuzzy.pca)
summary(mod.ads.eng.fuzzy.pca)
qqnorm(resid(mod.ads.eng.fuzzy.pca))

#### Strict
mod.ads.eng.strict.pca = lmer(logit.vs.percentage ~ PC.word.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.eng %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.eng.strict.pca)
summary(mod.ads.eng.strict.pca)
qqnorm(resid(mod.ads.eng.strict.pca))
```

### 3.2 Chintang
**Age**
```{r ads_chintang_age}
#### Fuzzy
mod.ads.cnt.fuzzy.age = lmer(logit.vs.percentage ~ age.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.cnt %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.cnt.fuzzy.age)
summary(mod.ads.cnt.fuzzy.age)
qqnorm(resid(mod.ads.cnt.fuzzy.age))

#### Strict
mod.ads.cnt.strict.age = lmer(logit.vs.percentage ~ age.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.cnt %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.cnt.strict.age)
summary(mod.ads.cnt.strict.age)
qqnorm(resid(mod.ads.cnt.strict.age))
```
  
**MLU**
```{r ads_chintang_mlu}
#### Fuzzy
mod.ads.cnt.fuzzy.mlu = lmer(logit.vs.percentage ~ MLUw.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.cnt %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e6))) 

anova(mod.ads.cnt.fuzzy.mlu)
summary(mod.ads.cnt.fuzzy.mlu)
qqnorm(resid(mod.ads.cnt.fuzzy.mlu))

#### Strict
mod.ads.cnt.strict.mlu = lmer(logit.vs.percentage ~ MLUw.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.cnt %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.cnt.strict.mlu)
summary(mod.ads.cnt.strict.mlu)
qqnorm(resid(mod.ads.cnt.strict.mlu))
```

**PCA**
```{r ads_chintang_pca}
#### Fuzzy
mod.ads.cnt.fuzzy.pca = lmer(logit.vs.percentage ~ PC.word.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.cnt %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.cnt.fuzzy.pca)
summary(mod.ads.cnt.fuzzy.pca)
qqnorm(resid(mod.ads.cnt.fuzzy.pca))

#### Strict
mod.ads.cnt.strict.pca = lmer(logit.vs.percentage ~ PC.word.groups + mean.nv.lens.words.scaled + window_size + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.cnt %>% filter(SD.outliers == "no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

anova(mod.ads.cnt.strict.pca)
summary(mod.ads.cnt.strict.pca)
qqnorm(resid(mod.ads.cnt.strict.pca))
```

## 4.0 Comparing against random baselines
These models include measurements from randomly simulated data as well as the original data. The goal of these analyses is to determine how and whether the patterns we have observed in Sections 1 and 2 differ from chance.  
  
We include two sets of analyses here. The first considers only CSS and looks at the data longitudinally. The second contrasts CSS and and ADS.  
  
### 4.1 Child-surrounding speech
This first set of analyses will replace the basic longitudinal analysis laid out in 2.0 in the text.
  
#### 4.1.1 English
**Age**
```{r rand_baseline_analysis_css_english_age}
# Words
###### Fuzzy
mod.words.fuzzy.age.rand.eng = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.age.rand.eng)
anova(mod.words.fuzzy.age.rand.eng)
mod.sums(mod.words.fuzzy.age.rand.eng, "../Results/mod.words.fuzzy.age.rand.eng.txt")

###### Strict
mod.words.strict.age.rand.eng = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.age.rand.eng)
anova(mod.words.strict.age.rand.eng)
mod.sums(mod.words.strict.age.rand.eng, "../Results/mod.words.strict.age.rand.eng.txt")
```
  
**MLU**
```{r rand_baseline_analysis_css_english_mlu}
###### Fuzzy
mod.words.fuzzy.mlu.rand.eng = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.mlu.rand.eng)
anova(mod.words.fuzzy.mlu.rand.eng)
mod.sums(mod.words.fuzzy.mlu.rand.eng, "../Results/mod.words.fuzzy.mlu.rand.eng.txt")

###### Strict
mod.words.strict.mlu.rand.eng = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.mlu.rand.eng)
anova(mod.words.strict.mlu.rand.eng)
mod.sums(mod.words.strict.mlu.rand.eng, "../Results/mod.words.strict.mlu.rand.eng.txt")
```

**PCA**
```{r rand_baseline_analysis_css_english_pca}
###### Fuzzy
mod.words.fuzzy.pca.rand.eng = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.pca.rand.eng)
anova(mod.words.fuzzy.pca.rand.eng)
mod.sums(mod.words.fuzzy.pca.rand.eng, "../Results/mod.words.fuzzy.pca.rand.eng.txt")

#### Strict
mod.words.strict.pca.rand.eng = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.pca.rand.eng)
anova(mod.words.strict.pca.rand.eng)
mod.sums(mod.words.strict.pca.rand.eng, "../Results/mod.words.strict.pca.rand.eng.txt")
```
  
**Entropy**
```{r rand_baseline_analysis_css_english_H}
###### Fuzzy
mod.words.fuzzy.ent.rand.eng = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.ent.rand.eng)
anova(mod.words.fuzzy.ent.rand.eng)
mod.sums(mod.words.fuzzy.ent.rand.eng, "../Results/mod.words.fuzzy.ent.rand.eng.txt")

#### Strict
mod.words.strict.ent.rand.eng = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "English_Manchester1" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.ent.rand.eng)
anova(mod.words.strict.ent.rand.eng)
mod.sums(mod.words.strict.ent.rand.eng, "../Results/mod.words.strict.ent.rand.eng.txt")
```
  

#### 4.1.2 Chintang
**Age**
```{r rand_baseline_analysis_css_chintang_age}
## Words
###### Fuzzy
mod.words.fuzzy.age.rand.cnt = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.age.rand.cnt)
anova(mod.words.fuzzy.age.rand.cnt)
mod.sums(mod.words.fuzzy.age.rand.cnt, "../Results/mod.words.fuzzy.age.rand.cnt.txt")

###### Strict
mod.words.strict.age.rand.cnt = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.age.rand.cnt)
anova(mod.words.strict.age.rand.cnt)
mod.sums(mod.words.strict.age.rand.cnt, "../Results/mod.words.strict.age.rand.cnt.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.age.rand.cnt = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.age.rand.cnt)
anova(mod.morph.fuzzy.age.rand.cnt)
mod.sums(mod.morph.fuzzy.age.rand.cnt, "../Results/mod.morph.fuzzy.age.rand.cnt.txt")

###### Strict
mod.morph.strict.age.rand.cnt = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.age.rand.cnt)
anova(mod.morph.strict.age.rand.cnt)
mod.sums(mod.morph.strict.age.rand.cnt, "../Results/mod.morph.strict.age.rand.cnt.txt")
```
  
**MLU**
```{r rand_baseline_analysis_css_chintang_mlu}
## Words
###### Fuzzy
mod.words.fuzzy.mlu.rand.cnt = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.mlu.rand.cnt)
anova(mod.words.fuzzy.mlu.rand.cnt)
mod.sums(mod.words.fuzzy.mlu.rand.cnt, "../Results/mod.words.fuzzy.mlu.rand.cnt.txt")

###### Strict
mod.words.strict.mlu.rand.cnt = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.mlu.rand.cnt)
anova(mod.words.strict.mlu.rand.cnt)
mod.sums(mod.words.strict.mlu.rand.cnt, "../Results/mod.words.strict.mlu.rand.cnt.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.mlu.rand.cnt = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.mlu.rand.cnt)
anova(mod.morph.fuzzy.mlu.rand.cnt)
mod.sums(mod.morph.fuzzy.mlu.rand.cnt, "../Results/mod.morph.fuzzy.mlu.rand.cnt.txt")

###### Strict
mod.morph.strict.mlu.rand.cnt = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.mlu.rand.cnt)
anova(mod.morph.strict.mlu.rand.cnt)
mod.sums(mod.morph.strict.mlu.rand.cnt, "../Results/mod.morph.strict.mlu.rand.cnt.txt")
```
  
**PCA**
```{r rand_baseline_analysis_css_chintang_pca}
## Words
###### Fuzzy
mod.words.fuzzy.pca.rand.cnt = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.pca.rand.cnt)
anova(mod.words.fuzzy.pca.rand.cnt)
mod.sums(mod.words.fuzzy.pca.rand.cnt, "../Results/mod.words.fuzzy.pca.rand.cnt.txt")

#### Strict
mod.words.strict.pca.rand.cnt = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.pca.rand.cnt)
anova(mod.words.strict.pca.rand.cnt)
mod.sums(mod.words.strict.pca.rand.cnt, "../Results/mod.words.strict.pca.rand.cnt.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.pca.rand.cnt = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.pca.rand.cnt)
anova(mod.morph.fuzzy.pca.rand.cnt)
mod.sums(mod.morph.fuzzy.pca.rand.cnt, "../Results/mod.morph.fuzzy.pca.rand.cnt.txt")

#### Strict
mod.morph.strict.pca.rand.cnt = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.pca.rand.cnt)
anova(mod.morph.strict.pca.rand.cnt)
mod.sums(mod.morph.strict.pca.rand.cnt, "../Results/mod.morph.strict.pca.rand.cnt.txt")
```
  
**Entropy**
```{r rand_baseline_analysis_css_chintang_H}
## Words
###### Fuzzy
mod.words.fuzzy.ent.rand.cnt = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.ent.rand.cnt)
anova(mod.words.fuzzy.ent.rand.cnt)
mod.sums(mod.words.fuzzy.ent.rand.cnt, "../Results/mod.words.fuzzy.ent.rand.cnt.txt")

#### Strict
mod.words.strict.ent.rand.cnt = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.ent.rand.cnt)
anova(mod.words.strict.ent.rand.cnt)
mod.sums(mod.words.strict.ent.rand.cnt, "../Results/mod.words.strict.ent.rand.cnt.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.ent.rand.cnt = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.ent.rand.cnt)
anova(mod.morph.fuzzy.ent.rand.cnt)
mod.sums(mod.morph.fuzzy.ent.rand.cnt, "../Results/mod.morph.fuzzy.ent.rand.cnt.txt")

#### Strict
mod.morph.strict.ent.rand.cnt = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Chintang" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.ent.rand.cnt)
anova(mod.morph.strict.ent.rand.cnt)
mod.sums(mod.morph.strict.ent.rand.cnt, "../Results/mod.morph.strict.ent.rand.cnt.txt")
```
  
#### 4.1.3 Inuktitut
*NB: Random effect structures are simplified due to model convergence issues.*  
**Age**
```{r rand_baseline_analysis_css_inuktitut_age}
## Words
###### Fuzzy
mod.words.fuzzy.age.rand.ink = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.fuzzy.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.age.rand.ink)
anova(mod.words.fuzzy.age.rand.ink)
mod.sums(mod.words.fuzzy.age.rand.ink, "../Results/mod.words.fuzzy.age.rand.ink.txt")

###### Strict
mod.words.strict.age.rand.ink = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.strict.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.age.rand.ink)
anova(mod.words.strict.age.rand.ink)
mod.sums(mod.words.strict.age.rand.ink, "../Results/mod.words.strict.age.rand.ink.txt")

## Morphemes
###### Fuzzy (simplified)
mod.morph.fuzzy.age.rand.ink = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.fuzzy.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e6)))

summary(mod.morph.fuzzy.age.rand.ink)
anova(mod.morph.fuzzy.age.rand.ink)
mod.sums(mod.morph.fuzzy.age.rand.ink, "../Results/mod.morph.fuzzy.age.rand.ink.txt")

###### Strict (simplified)
mod.morph.strict.age.rand.ink = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.strict.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.age.rand.ink)
anova(mod.morph.strict.age.rand.ink)
mod.sums(mod.morph.fuzzy.age.rand.ink, "../Results/mod.morph.fuzzy.age.rand.ink.txt")
```
  
**MLU**
```{r rand_baseline_analysis_css_inuktitut_mlu}
## Words
###### Fuzzy
mod.words.fuzzy.mlu.rand.ink = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.fuzzy.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.mlu.rand.ink)
anova(mod.words.fuzzy.mlu.rand.ink)
mod.sums(mod.words.fuzzy.mlu.rand.ink, "../Results/mod.words.fuzzy.mlu.rand.ink.txt")

###### Strict
mod.words.strict.mlu.rand.ink = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.strict.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.mlu.rand.ink)
anova(mod.words.strict.mlu.rand.ink)
mod.sums(mod.words.strict.mlu.rand.ink, "../Results/mod.words.strict.mlu.rand.ink.txt")

## Morphemes
###### Fuzzy (simplified)
mod.morph.fuzzy.mlu.rand.ink = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.fuzzy.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.mlu.rand.ink)
anova(mod.morph.fuzzy.mlu.rand.ink)
mod.sums(mod.morph.fuzzy.mlu.rand.ink, "../Results/mod.morph.fuzzy.mlu.rand.ink.txt")

###### Strict (simplified)
mod.morph.strict.mlu.rand.ink = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.strict.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.mlu.rand.ink)
anova(mod.morph.strict.mlu.rand.ink)
```
  
**PCA**
```{r rand_baseline_analysis_css_inuktitut_pca}
## Words
###### Fuzzy
mod.words.fuzzy.pca.rand.ink = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.fuzzy.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.pca.rand.ink)
anova(mod.words.fuzzy.pca.rand.ink)
mod.sums(mod.words.fuzzy.pca.rand.ink, "../Results/mod.words.fuzzy.pca.rand.ink.txt")

#### Strict
mod.words.strict.pca.rand.ink = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.strict.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.pca.rand.ink)
anova(mod.words.strict.pca.rand.ink)
mod.sums(mod.words.strict.pca.rand.ink, "../Results/mod.words.strict.pca.rand.ink.txt")

## Morphemes
###### Fuzzy (simplified)
mod.morph.fuzzy.pca.rand.ink = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.fuzzy.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.pca.rand.ink)
anova(mod.morph.fuzzy.pca.rand.ink)
mod.sums(mod.morph.fuzzy.pca.rand.ink, "../Results/mod.morph.fuzzy.pca.rand.ink.txt")

#### Strict (simplified)
mod.morph.strict.pca.rand.ink = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.strict.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.pca.rand.ink)
anova(mod.morph.strict.pca.rand.ink)
mod.sums(mod.morph.strict.pca.rand.ink, "../Results/mod.morph.strict.pca.rand.ink.txt")
```
  
**Entropy**
```{r rand_baseline_analysis_css_inuktitut_H}
## Words
###### Fuzzy
mod.words.fuzzy.ent.rand.ink = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.fuzzy.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.ent.rand.ink)
anova(mod.words.fuzzy.ent.rand.ink)
mod.sums(mod.words.fuzzy.ent.rand.ink, "../Results/mod.words.fuzzy.ent.rand.ink.txt")

#### Strict
mod.words.strict.ent.rand.ink = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.strict.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.ent.rand.ink)
anova(mod.words.strict.ent.rand.ink)
mod.sums(mod.words.strict.ent.rand.ink, "../Results/mod.words.strict.ent.rand.ink.txt")

## Morphemes
###### Fuzzy (simplfied)
mod.morph.fuzzy.ent.rand.ink = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.fuzzy.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.ent.rand.ink)
anova(mod.morph.fuzzy.ent.rand.ink)
mod.sums(mod.morph.fuzzy.ent.rand.ink, "../Results/mod.morph.fuzzy.ent.rand.ink.txt")

#### Strict (simplified)
mod.morph.strict.ent.rand.ink = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.strict.rand %>% filter(corpus == "Inuktitut" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.ent.rand.ink)
anova(mod.morph.strict.ent.rand.ink)
mod.sums(mod.morph.strict.ent.rand.ink, "../Results/mod.morph.strict.ent.rand.ink.txt")
```

########################################
#### 4.1.4 Japanese
**Age**
```{r rand_baseline_analysis_css_japanese_age}
## Words
###### Fuzzy
mod.words.fuzzy.age.rand.jpn = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.age.rand.jpn)
anova(mod.words.fuzzy.age.rand.jpn)
mod.sums(mod.words.fuzzy.age.rand.jpn, "../Results/mod.words.fuzzy.age.rand.jpn.txt")

###### Strict
mod.words.strict.age.rand.jpn = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.age.rand.jpn)
anova(mod.words.strict.age.rand.jpn)
mod.sums(mod.words.strict.age.rand.jpn, "../Results/mod.words.strict.age.rand.jpn.txt")
```
  
**MLU**
```{r rand_baseline_analysis_css_japanese_mlu}
## Words
###### Fuzzy
mod.words.fuzzy.mlu.rand.jpn = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.mlu.rand.jpn)
anova(mod.words.fuzzy.mlu.rand.jpn)
mod.sums(mod.words.fuzzy.mlu.rand.jpn, "../Results/mod.words.fuzzy.mlu.rand.jpn.txt")

###### Strict
mod.words.strict.mlu.rand.jpn = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.mlu.rand.jpn)
anova(mod.words.strict.mlu.rand.jpn)
mod.sums(mod.words.strict.mlu.rand.jpn, "../Results/mod.words.strict.mlu.rand.jpn.txt")
```
  
**PCA**
```{r rand_baseline_analysis_css_japanese_pca}
## Words
###### Fuzzy
###### NB: Needed to simplify random effect structure to get convergence.
mod.words.fuzzy.pca.rand.jpn = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.word.fuzzy.rand %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.pca.rand.jpn)
anova(mod.words.fuzzy.pca.rand.jpn)
mod.sums(mod.words.fuzzy.pca.rand.jpn, "../Results/mod.words.fuzzy.pca.rand.jpn.txt")

#### Strict
mod.words.strict.pca.rand.jpn = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.pca.rand.jpn)
anova(mod.words.strict.pca.rand.jpn)
mod.sums(mod.words.strict.pca.rand.jpn, "../Results/mod.words.strict.pca.rand.jpn.txt")
```
  
**Entropy**
```{r rand_baseline_analysis_css_japanese_H}
## Words
###### Fuzzy
mod.words.fuzzy.ent.rand.jpn = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.ent.rand.jpn)
anova(mod.words.fuzzy.ent.rand.jpn)
mod.sums(mod.words.fuzzy.ent.rand.jpn, "../Results/mod.words.fuzzy.ent.rand.jpn.txt")

#### Strict
mod.words.strict.ent.rand.jpn = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Japanese_MiiPro" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.ent.rand.jpn)
anova(mod.words.strict.ent.rand.jpn)
mod.sums(mod.words.strict.ent.rand.jpn, "../Results/mod.words.strict.ent.rand.jpn.txt")
```

#### 4.1.5 Russian
**Age**
```{r rand_baseline_analysis_css_russian_age}
## Words
###### Fuzzy
mod.words.fuzzy.age.rand.rus = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.age.rand.rus)
anova(mod.words.fuzzy.age.rand.rus)
mod.sums(mod.words.fuzzy.age.rand.rus, "../Results/mmod.words.fuzzy.age.rand.rus.txt")

###### Strict
mod.words.strict.age.rand.rus = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.age.rand.rus)
anova(mod.words.strict.age.rand.rus)
mod.sums(mod.words.strict.age.rand.rus, "../Results/mod.words.strict.age.rand.rus.txt")

```
  
**MLU**
```{r rand_baseline_analysis_css_russian_mlu}
## Words
###### Fuzzy
mod.words.fuzzy.mlu.rand.rus = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.mlu.rand.rus)
anova(mod.words.fuzzy.mlu.rand.rus)
mod.sums(mod.words.fuzzy.mlu.rand.rus, "../Results/mod.words.fuzzy.mlu.rand.rus.txt")

###### Strict
mod.words.strict.mlu.rand.rus = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.mlu.rand.rus)
anova(mod.words.strict.mlu.rand.rus)
mod.sums(mod.words.strict.mlu.rand.rus, "../Results/mod.words.strict.mlu.rand.rus.txt")

```
  
**PCA**
```{r rand_baseline_analysis_css_russian_pca}
## Words
###### Fuzzy
mod.words.fuzzy.pca.rand.rus = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.pca.rand.rus)
anova(mod.words.fuzzy.pca.rand.rus)
mod.sums(mod.words.fuzzy.age.rand.jpn, "../Results/mod.words.fuzzy.age.rand.jpn.txt")

#### Strict
mod.words.strict.pca.rand.rus = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.pca.rand.rus)
anova(mod.words.strict.pca.rand.rus)
mod.sums(mod.words.fuzzy.age.rand.jpn, "../Results/mod.words.fuzzy.age.rand.jpn.txt")

```
  
**Entropy**
```{r rand_baseline_analysis_css_russian_H}
## Words
###### Fuzzy
mod.words.fuzzy.ent.rand.rus = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.ent.rand.rus)
anova(mod.words.fuzzy.ent.rand.rus)
mod.sums(mod.words.fuzzy.age.rand.jpn, "../Results/mod.words.fuzzy.age.rand.jpn.txt")

#### Strict
mod.words.strict.ent.rand.rus = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Russian" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.ent.rand.rus)
anova(mod.words.strict.ent.rand.rus)
mod.sums(mod.words.fuzzy.age.rand.jpn, "../Results/mod.words.fuzzy.age.rand.jpn.txt")

```
  
#### 4.1.6 Sesotho
**Age**
```{r rand_baseline_analysis_css_sesotho_age}
## Words
###### Fuzzy
mod.words.fuzzy.age.rand.ses = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.age.rand.ses)
anova(mod.words.fuzzy.age.rand.ses)
mod.sums(mod.words.fuzzy.age.rand.ses, "../Results/mod.words.fuzzy.age.rand.ses.txt")

###### Strict
mod.words.strict.age.rand.ses = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.age.rand.ses)
anova(mod.words.strict.age.rand.ses)
mod.sums(mod.words.strict.age.rand.ses, "../Results/mod.words.strict.age.rand.ses.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.age.rand.ses = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.age.rand.ses)
anova(mod.morph.fuzzy.age.rand.ses)
mod.sums(mod.morph.fuzzy.age.rand.ses, "../Results/mod.morph.fuzzy.age.rand.ses.txt")

###### Strict
mod.morph.strict.age.rand.ses = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.age.rand.ses)
anova(mod.morph.strict.age.rand.ses)
mod.sums(mod.morph.strict.age.rand.ses, "../Results/mod.morph.strict.age.rand.ses.txt")

```
  
**MLU**
```{r rand_baseline_analysis_css_sesotho_mlu}
## Words
###### Fuzzy
mod.words.fuzzy.mlu.rand.ses = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.mlu.rand.ses)
anova(mod.words.fuzzy.mlu.rand.ses)
mod.sums(mod.words.fuzzy.mlu.rand.ses, "../Results/mod.words.fuzzy.mlu.rand.ses.txt")

###### Strict
mod.words.strict.mlu.rand.ses = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.mlu.rand.ses)
anova(mod.words.strict.mlu.rand.ses)
mod.sums(mod.words.strict.mlu.rand.ses, "../Results/mod.words.strict.mlu.rand.ses.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.mlu.rand.ses = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.mlu.rand.ses)
anova(mod.morph.fuzzy.mlu.rand.ses)
mod.sums(mod.morph.fuzzy.mlu.rand.ses, "../Results/mod.morph.fuzzy.mlu.rand.ses.txt")

###### Strict
mod.morph.strict.mlu.rand.ses = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.mlu.rand.ses)
anova(mod.morph.strict.mlu.rand.ses)
mod.sums(mod.morph.strict.mlu.rand.ses, "../Results/mod.morph.strict.mlu.rand.ses.txt")

```
  
**PCA**
```{r rand_baseline_analysis_css_sesotho_pca}
## Words
###### Fuzzy
mod.words.fuzzy.pca.rand.ses = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.pca.rand.ses)
anova(mod.words.fuzzy.pca.rand.ses)
mod.sums(mod.words.fuzzy.pca.rand.ses, "../Results/mod.words.fuzzy.pca.rand.ses.txt")

#### Strict
mod.words.strict.pca.rand.ses = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.pca.rand.ses)
anova(mod.words.strict.pca.rand.ses)
mod.sums(mod.words.strict.pca.rand.ses, "../Results/mod.words.strict.pca.rand.ses.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.pca.rand.ses = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.pca.rand.ses)
anova(mod.morph.fuzzy.pca.rand.ses)
mod.sums(mod.morph.fuzzy.pca.rand.ses, "../Results/mod.morph.fuzzy.pca.rand.ses.txt")

#### Strict
mod.morph.strict.pca.rand.ses = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.pca.rand.ses)
anova(mod.morph.strict.pca.rand.ses)
mod.sums(mod.morph.strict.pca.rand.ses, "../Results/mod.morph.strict.pca.rand.ses.txt")

```
  
**Entropy**
```{r rand_baseline_analysis_css_sesotho_H}
## Words
###### Fuzzy
mod.words.fuzzy.ent.rand.ses = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.ent.rand.ses)
anova(mod.words.fuzzy.ent.rand.ses)
mod.sums(mod.words.fuzzy.ent.rand.ses, "../Results/mod.words.fuzzy.ent.rand.ses.txt")

#### Strict
mod.words.strict.ent.rand.ses = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.ent.rand.ses)
anova(mod.words.strict.ent.rand.ses)
mod.sums(mod.words.strict.ent.rand.ses, "../Results/mod.words.strict.ent.rand.ses.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.ent.rand.ses = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.ent.rand.ses)
anova(mod.morph.fuzzy.ent.rand.ses)
mod.sums(mod.morph.fuzzy.ent.rand.ses, "../Results/mod.morph.fuzzy.ent.rand.ses.txt")

#### Strict
mod.morph.strict.ent.rand.ses = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Sesotho" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.ent.rand.ses)
anova(mod.morph.strict.ent.rand.ses)
mod.sums(mod.morph.strict.ent.rand.ses, "../Results/mod.morph.strict.ent.rand.ses.txt")

```

#### 4.1.7 Turkish
**Age**
```{r rand_baseline_analysis_css_turkish_age}
## Words
###### Fuzzy
mod.words.fuzzy.age.rand.tur = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.age.rand.tur)
anova(mod.words.fuzzy.age.rand.tur)
mod.sums(mod.words.fuzzy.age.rand.tur, "../Results/mod.words.fuzzy.age.rand.tur.txt")

###### Strict
mod.words.strict.age.rand.tur = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.age.rand.tur)
anova(mod.words.strict.age.rand.tur)
mod.sums(mod.words.strict.age.rand.tur, "../Results/mod.words.strict.age.rand.tur.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.age.rand.tur = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.age.rand.tur)
anova(mod.morph.fuzzy.age.rand.tur)
mod.sums(mod.morph.fuzzy.age.rand.tur, "../Results/mod.morph.fuzzy.age.rand.tur.txt")

###### Strict
mod.morph.strict.age.rand.tur = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.age.rand.tur)
anova(mod.morph.strict.age.rand.tur)
mod.sums(mod.morph.strict.age.rand.tur, "../Results/mod.morph.strict.age.rand.tur.txt")

```
  
**MLU**
```{r rand_baseline_analysis_css_turkish_mlu}
## Words
###### Fuzzy
mod.words.fuzzy.mlu.rand.tur = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.mlu.rand.tur)
anova(mod.words.fuzzy.mlu.rand.tur)
mod.sums(mod.words.fuzzy.mlu.rand.tur, "../Results/mod.words.fuzzy.mlu.rand.tur.txt")

###### Strict
mod.words.strict.mlu.rand.tur = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.mlu.rand.tur)
anova(mod.words.strict.mlu.rand.tur)
mod.sums(mod.words.strict.mlu.rand.tur, "../Results/mod.words.strict.mlu.rand.tur.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.mlu.rand.tur = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.mlu.rand.tur)
anova(mod.morph.fuzzy.mlu.rand.tur)
mod.sums(mod.morph.fuzzy.mlu.rand.tur, "../Results/mod.morph.fuzzy.mlu.rand.tur.txt")

###### Strict
mod.morph.strict.mlu.rand.tur = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.mlu.rand.tur)
anova(mod.morph.strict.mlu.rand.tur)
mod.sums(mod.morph.strict.mlu.rand.tur, "../Results/mod.morph.strict.mlu.rand.tur.txt")

```
  
**PCA**
```{r rand_baseline_analysis_css_turkish_pca}
## Words
###### Fuzzy
mod.words.fuzzy.pca.rand.tur = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.pca.rand.tur)
anova(mod.words.fuzzy.pca.rand.tur)
mod.sums(mod.words.fuzzy.pca.rand.tur, "../Results/mod.words.fuzzy.pca.rand.tur.txt")

#### Strict
mod.words.strict.pca.rand.tur = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.pca.rand.tur)
anova(mod.words.strict.pca.rand.tur)
mod.sums(mod.words.strict.pca.rand.tur, "../Results/mod.words.strict.pca.rand.tur.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.pca.rand.tur = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.pca.rand.tur)
anova(mod.morph.fuzzy.pca.rand.tur)
mod.sums(mod.morph.fuzzy.pca.rand.tur, "../Results/mod.morph.fuzzy.pca.rand.tur.txt")

#### Strict
mod.morph.strict.pca.rand.tur = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.pca.rand.tur)
anova(mod.morph.strict.pca.rand.tur)
mod.sums(mod.morph.strict.pca.rand.tur, "../Results/mod.morph.strict.pca.rand.tur.txt")

```
  
**Entropy**
```{r rand_baseline_analysis_css_turkish_H}
## Words
###### Fuzzy
mod.words.fuzzy.ent.rand.tur = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.ent.rand.tur)
anova(mod.words.fuzzy.ent.rand.tur)
mod.sums(mod.words.fuzzy.ent.rand.tur, "../Results/mod.words.fuzzy.ent.rand.tur.txt")

#### Strict
mod.words.strict.ent.rand.tur = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.ent.rand.tur)
anova(mod.words.strict.ent.rand.tur)
mod.sums(mod.words.strict.ent.rand.tur, "../Results/mod.words.strict.ent.rand.tur.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.ent.rand.tur = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.ent.rand.tur)
anova(mod.morph.fuzzy.ent.rand.tur)
mod.sums(mod.morph.fuzzy.ent.rand.tur, "../Results/mod.morph.fuzzy.ent.rand.tur.txt")

#### Strict
mod.morph.strict.ent.rand.tur = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Turkish" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.ent.rand.tur)
anova(mod.morph.strict.ent.rand.tur)
mod.sums(mod.morph.strict.ent.rand.tur, "../Results/mod.morph.strict.ent.rand.tur.txt")

```
  
#### 4.1.8 Yucatec
**Age**
```{r rand_baseline_analysis_css_yucatec_age}
## Words
###### Fuzzy
mod.words.fuzzy.age.rand.yuc = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.age.rand.yuc)
anova(mod.words.fuzzy.age.rand.yuc)
mod.sums(mod.words.fuzzy.age.rand.yuc, "../Results/mod.words.fuzzy.age.rand.yuc.txt")

###### Strict
mod.words.strict.age.rand.yuc = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.age.rand.yuc)
anova(mod.words.strict.age.rand.yuc)
mod.sums(mod.words.strict.age.rand.yuc, "../Results/mod.words.strict.age.rand.yuc.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.age.rand.yuc = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.age.rand.yuc)
anova(mod.morph.fuzzy.age.rand.yuc)
mod.sums(mod.morph.fuzzy.age.rand.yuc, "../Results/mod.morph.fuzzy.age.rand.yuc.txt")

###### Strict (simplified)
mod.morph.strict.age.rand.yuc = lmer(logit.vs.percentage ~ random*logAge.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.strict.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.age.rand.yuc)
anova(mod.morph.strict.age.rand.yuc)
mod.sums(mod.morph.strict.age.rand.yuc, "../Results/mod.morph.strict.age.rand.yuc.txt")

```

**MLU**
```{r rand_baseline_analysis_css_yucatec_mlu}
## Words
###### Fuzzy
mod.words.fuzzy.mlu.rand.yuc = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.mlu.rand.yuc)
anova(mod.words.fuzzy.mlu.rand.yuc)
mod.sums(mod.words.fuzzy.mlu.rand.yuc, "../Results/mod.words.fuzzy.mlu.rand.yuc.txt")

###### Strict
mod.words.strict.mlu.rand.yuc = lmer(logit.vs.percentage ~ random*MLUw.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.mlu.rand.yuc)
anova(mod.words.strict.mlu.rand.yuc)
mod.sums(mod.words.strict.mlu.rand.yuc, "../Results/mod.words.strict.mlu.rand.yuc.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.mlu.rand.yuc = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.mlu.rand.yuc)
anova(mod.morph.fuzzy.mlu.rand.yuc)
mod.sums(mod.morph.fuzzy.mlu.rand.yuc, "../Results/mod.morph.fuzzy.mlu.rand.yuc.txt")

###### Strict (simplified)
mod.morph.strict.mlu.rand.yuc = lmer(logit.vs.percentage ~ random*MLUm.scaled + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.strict.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.mlu.rand.yuc)
anova(mod.morph.strict.mlu.rand.yuc)
mod.sums(mod.morph.strict.mlu.rand.yuc, "../Results/mod.morph.strict.mlu.rand.yuc.txt")

```

**PCA**
```{r rand_baseline_analysis_css_yucatec_pca}
## Words
###### Fuzzy
mod.words.fuzzy.pca.rand.yuc = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.pca.rand.yuc)
anova(mod.words.fuzzy.pca.rand.yuc)
mod.sums(mod.words.fuzzy.pca.rand.yuc, "../Results/mod.words.fuzzy.pca.rand.yuc.txt")

#### Strict
mod.words.strict.pca.rand.yuc = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.pca.rand.yuc)
anova(mod.words.strict.pca.rand.yuc)
mod.sums(mod.words.strict.pca.rand.yuc, "../Results/mod.words.strict.pca.rand.yuc.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.pca.rand.yuc = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.pca.rand.yuc)
anova(mod.morph.fuzzy.pca.rand.yuc)
mod.sums(mod.morph.fuzzy.pca.rand.yuc, "../Results/mod.morph.fuzzy.pca.rand.yuc.txt")

#### Strict (simplified)
mod.morph.strict.pca.rand.yuc = lmer(logit.vs.percentage ~ random*PC1_word + window_size + mean.nv.lens.morphs.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child), data = css.morph.strict.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.pca.rand.yuc)
anova(mod.morph.strict.pca.rand.yuc)
mod.sums(mod.morph.strict.pca.rand.yuc, "../Results/mod.morph.strict.pca.rand.yuc.txt")

```
  
**Entropy**
```{r rand_baseline_analysis_css_yucatec_H}
## Words
###### Fuzzy
mod.words.fuzzy.ent.rand.yuc = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.fuzzy.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.fuzzy.ent.rand.yuc)
anova(mod.words.fuzzy.ent.rand.yuc)
mod.sums(mod.words.fuzzy.ent.rand.yuc, "../Results/mod.words.fuzzy.ent.rand.yuc.txt")

#### Strict
mod.words.strict.ent.rand.yuc = lmer(logit.vs.percentage ~ random*WordEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.word.strict.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.words.strict.ent.rand.yuc)
anova(mod.words.strict.ent.rand.yuc)
mod.sums(mod.words.strict.ent.rand.yuc, "../Results/mod.words.strict.ent.rand.yuc.txt")

## Morphemes
###### Fuzzy
mod.morph.fuzzy.ent.rand.yuc = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.fuzzy.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.fuzzy.ent.rand.yuc)
anova(mod.morph.fuzzy.ent.rand.yuc)
mod.sums(mod.morph.fuzzy.ent.rand.yuc, "../Results/mod.morph.fuzzy.ent.rand.yuc.txt")

#### Strict
mod.morph.strict.ent.rand.yuc = lmer(logit.vs.percentage ~ random*MorphEnt.scaled + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled +  (1|target.child/session_id), data = css.morph.strict.rand %>% filter(corpus == "Yucatec" & SD.outliers=="no"), control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))

summary(mod.morph.strict.ent.rand.yuc)
anova(mod.morph.strict.ent.rand.yuc)
mod.sums(mod.morph.strict.ent.rand.yuc, "../Results/mod.morph.strict.ent.rand.yuc.txt")

```

#### 4.1.9 Plotting
##### 4.1.9.1 Some plotting functions
```{r plotting_rand_css_functions}
library(effects)
p.finder = function(x){
    if(x<=.001){
        stars = "***"
    }
    else if(x > .001 & x <= .01){
        stars = "**"
    }
    else if(x > .01 & x <= .05){
        stars = "*"
    }
    else{
        stars = ""
    }    
    return(stars)
}

rand.plotter = function(mod, simp.mod, title, legend=F){
    orig.p = anova(simp.mod)[[6]][1]
    eff.obj = allEffects(mod)[5][[1]]
    anova.obj = anova(mod)
    p.val = anova.obj[[6]][7]
    stars.orig = p.finder(orig.p)
    stars.orig = ifelse(stars.orig == "", "n.s.", stars.orig)
    stars.int = p.finder(p.val)
    eff.matrix = as.data.frame(eff.obj$model.matrix)
    eff.fit = bind_cols(eff.matrix,
                        fit = ilogit(eff.obj$fit),
                        lower = ilogit(eff.obj$lower),
                        upper = ilogit(eff.obj$upper)) %>%
                        rename(random = randomrandom) %>%
                        mutate(random = ifelse(random==0, "original", "random"))
    p = ggplot(eff.fit, aes(y = fit, x = eff.fit[,3], color=random, fill=random)) +
               geom_ribbon(aes(ymin=lower, ymax = upper, color=random),
                           alpha=0.1,
                           linetype=1) +
                geom_line(lwd = 1.5) + 
                theme_bw() +
                ylim(0,1) +
                labs(fill = "Text type", color = "Text type") + 
                ggtitle(paste0(title, stars.int, " (", stars.orig, ")")) +
                xlab(element_blank()) +
                ylab(element_blank()) + 
                theme(plot.title = element_text(hjust=0.5),
                      text = element_text(size = 10, family="Times New Roman", face="bold")) + 
                scale_fill_manual(values = c("dodgerblue", "darkred")) +
                scale_color_manual(values = c("dodgerblue", "darkred"))
    if(!legend){
        p = p + theme(legend.position = "none")
    }
    else{
        p = p + theme(legend.position = "right")
    }
    return(p)
}

rand.row.collector = function(plot.list, label){
    arr.grid = ggarrange(plotlist = plot.list, ncol=4)
    p = annotate_figure(arr.grid, left = text_grob(label, family="Times New Roman", face="bold"))
    return(p)
}
```
  
##### 4.1.9.2 Collecting plots for collating
We first generate and save plots for each individual language and developmental index. These will later be combined into multi-panel plots.
###### 4.1.9.2.1 Chintang
```{r get_plots_chintang}
## Words
#### Age
###### Fuzzy
cnt.word.fuzzy.age.plot = rand.plotter(mod.words.fuzzy.age.rand.cnt, cnt.mod.w.f.a, "Age")

###### Strict
cnt.word.strict.age.plot = rand.plotter(mod.words.strict.age.rand.cnt, cnt.mod.w.s.a, "Age")

#### MLU
###### Fuzzy
cnt.word.fuzzy.mlu.plot = rand.plotter(mod.words.fuzzy.mlu.rand.cnt, cnt.mod.w.f.m, "MLU")

###### Strict
cnt.word.strict.mlu.plot = rand.plotter(mod.words.strict.mlu.rand.cnt, cnt.mod.w.s.m, "MLU")

#### PCA
###### Fuzzy
cnt.word.fuzzy.pca.plot = rand.plotter(mod.words.fuzzy.pca.rand.cnt, cnt.mod.w.f.p, "PCA")

###### Strict
cnt.word.strict.pca.plot = rand.plotter(mod.words.strict.pca.rand.cnt, cnt.mod.w.s.p, "PCA")

#### Entropy
###### Fuzzy
cnt.word.fuzzy.ent.plot = rand.plotter(mod.words.fuzzy.ent.rand.cnt, cnt.mod.w.f.h, "Lexical H")

###### Strict
cnt.word.strict.ent.plot = rand.plotter(mod.words.strict.ent.rand.cnt, cnt.mod.w.s.h, "Lexical H")

## Morphemes
#### Age
###### Fuzzy
cnt.morph.fuzzy.age.plot = rand.plotter(mod.morph.fuzzy.age.rand.cnt, cnt.mod.m.f.a, "Age")

###### Strict
cnt.morph.strict.age.plot = rand.plotter(mod.morph.strict.age.rand.cnt, cnt.mod.m.s.a, "Age")

#### MLU
###### Fuzzy
cnt.morph.fuzzy.mlu.plot = rand.plotter(mod.morph.fuzzy.mlu.rand.cnt, cnt.mod.m.f.m, "MLU")

###### Strict
cnt.morph.strict.mlu.plot = rand.plotter(mod.morph.strict.mlu.rand.cnt, cnt.mod.m.s.m, "MLU")

#### PCA
###### Fuzzy
cnt.morph.fuzzy.pca.plot = rand.plotter(mod.morph.fuzzy.pca.rand.cnt, cnt.mod.m.f.p, "PCA")

###### Strict
cnt.morph.strict.pca.plot = rand.plotter(mod.morph.strict.pca.rand.cnt, cnt.mod.m.s.p, "PCA")

#### Entropy
###### Fuzzy
cnt.morph.fuzzy.ent.plot = rand.plotter(mod.morph.fuzzy.ent.rand.cnt, cnt.mod.m.f.h, "Morph. H")

###### Strict
cnt.morph.strict.ent.plot = rand.plotter(mod.morph.strict.ent.rand.cnt, cnt.mod.m.s.h, "Morph. H")

#### create plot lists
cnt.word.fuzzy.list = list(cnt.word.fuzzy.age.plot,
                           cnt.word.fuzzy.mlu.plot,
                           cnt.word.fuzzy.ent.plot,
                           cnt.word.fuzzy.pca.plot)

cnt.word.strict.list = list(cnt.word.strict.age.plot,
                            cnt.word.strict.mlu.plot,
                            cnt.word.strict.ent.plot,
                            cnt.word.strict.pca.plot)

cnt.morph.fuzzy.list = list(cnt.morph.fuzzy.age.plot,
                            cnt.morph.fuzzy.mlu.plot,
                            cnt.morph.fuzzy.ent.plot,
                            cnt.morph.fuzzy.pca.plot)

cnt.morph.strict.list = list(cnt.morph.strict.age.plot,
                             cnt.morph.strict.mlu.plot,
                             cnt.morph.strict.ent.plot,
                             cnt.morph.strict.pca.plot)
```
  
###### 4.1.9.2.2 English
```{r get_plots_english}
## Words
#### Age
###### Fuzzy
eng.word.fuzzy.age.plot = rand.plotter(mod.words.fuzzy.age.rand.eng, eng.mod.w.f.a, "Age")

###### Strict
eng.word.strict.age.plot = rand.plotter(mod.words.strict.age.rand.eng, eng.mod.w.s.a, "Age")

#### MLU
###### Fuzzy
eng.word.fuzzy.mlu.plot = rand.plotter(mod.words.fuzzy.mlu.rand.eng, eng.mod.w.f.m, "MLU")

###### Strict
eng.word.strict.mlu.plot = rand.plotter(mod.words.strict.mlu.rand.eng, eng.mod.w.s.m, "MLU")

#### PCA
###### Fuzzy
eng.word.fuzzy.pca.plot = rand.plotter(mod.words.fuzzy.pca.rand.eng, eng.mod.w.f.p, "PCA")

###### Strict
eng.word.strict.pca.plot = rand.plotter(mod.words.strict.pca.rand.eng, eng.mod.w.s.p, "PCA")

#### Entropy
###### Fuzzy
eng.word.fuzzy.ent.plot = rand.plotter(mod.words.fuzzy.ent.rand.eng, eng.mod.w.f.h, "Lexical H")

###### Strict
eng.word.strict.ent.plot = rand.plotter(mod.words.strict.ent.rand.eng, eng.mod.w.s.h, "Lexical H")

# create plot lists
eng.word.fuzzy.list = list(eng.word.fuzzy.age.plot,
                           eng.word.fuzzy.mlu.plot,
                           eng.word.fuzzy.ent.plot,
                           eng.word.fuzzy.pca.plot)

eng.word.strict.list = list(eng.word.strict.age.plot,
                            eng.word.strict.mlu.plot,
                            eng.word.strict.ent.plot,
                            eng.word.strict.pca.plot)

```
  
###### 4.1.9.2.3 Inuktitut
```{r get_plots_inuktitut}
## Words
#### Age
###### Fuzzy
ink.word.fuzzy.age.plot = rand.plotter(mod.words.fuzzy.age.rand.ink, ink.mod.w.f.a, "Age")

###### Strict
ink.word.strict.age.plot = rand.plotter(mod.words.strict.age.rand.ink, ink.mod.w.s.a, "Age")

#### MLU
###### Fuzzy
ink.word.fuzzy.mlu.plot = rand.plotter(mod.words.fuzzy.mlu.rand.ink, ink.mod.w.f.m, "MLU")

###### Strict
ink.word.strict.mlu.plot = rand.plotter(mod.words.strict.mlu.rand.ink, ink.mod.w.s.m, "MLU")

#### PCA
###### Fuzzy
ink.word.fuzzy.pca.plot = rand.plotter(mod.words.fuzzy.pca.rand.ink, ink.mod.w.f.p, "PCA")

###### Strict
ink.word.strict.pca.plot = rand.plotter(mod.words.strict.pca.rand.ink, ink.mod.w.s.p, "PCA")

#### Entropy
###### Fuzzy
ink.word.fuzzy.ent.plot = rand.plotter(mod.words.fuzzy.ent.rand.ink, ink.mod.w.f.h, "Lexical H")

###### Strict
ink.word.strict.ent.plot = rand.plotter(mod.words.strict.ent.rand.ink, ink.mod.w.s.h, "Lexical H")

## Morphemes
#### Age
###### Fuzzy
ink.morph.fuzzy.age.plot = rand.plotter(mod.morph.fuzzy.age.rand.ink, ink.mod.m.f.a, "Age")

###### Strict
ink.morph.strict.age.plot = rand.plotter(mod.morph.strict.age.rand.ink, ink.mod.m.s.a, "Age")

#### MLU
###### Fuzzy
ink.morph.fuzzy.mlu.plot = rand.plotter(mod.morph.fuzzy.mlu.rand.ink, ink.mod.m.f.m, "MLU")

###### Strict
ink.morph.strict.mlu.plot = rand.plotter(mod.morph.strict.mlu.rand.ink, ink.mod.m.s.m, "MLU")

#### PCA
###### Fuzzy
ink.morph.fuzzy.pca.plot = rand.plotter(mod.morph.fuzzy.pca.rand.ink, ink.mod.m.f.p, "PCA")

###### Strict
ink.morph.strict.pca.plot = rand.plotter(mod.morph.strict.pca.rand.ink, ink.mod.m.s.p, "PCA")

#### Entropy
###### Fuzzy
ink.morph.fuzzy.ent.plot = rand.plotter(mod.morph.fuzzy.ent.rand.ink, ink.mod.m.f.h, "Morph. H")

###### Strict
ink.morph.strict.ent.plot = rand.plotter(mod.morph.strict.ent.rand.ink, ink.mod.m.s.h, "Morph. H")

#### construct plot lists
ink.word.fuzzy.list = list(ink.word.fuzzy.age.plot,
                           ink.word.fuzzy.mlu.plot,
                           ink.word.fuzzy.ent.plot,
                           ink.word.fuzzy.pca.plot)

ink.word.strict.list = list(ink.word.strict.age.plot,
                            ink.word.strict.mlu.plot,
                            ink.word.strict.ent.plot,
                            ink.word.strict.pca.plot)

ink.morph.fuzzy.list = list(ink.morph.fuzzy.age.plot,
                            ink.morph.fuzzy.mlu.plot,
                            ink.morph.fuzzy.ent.plot,
                            ink.morph.fuzzy.pca.plot)

ink.morph.strict.list = list(ink.morph.strict.age.plot,
                             ink.morph.strict.mlu.plot,
                             ink.morph.strict.ent.plot,
                             ink.morph.strict.pca.plot)
```
  
###### 4.1.9.2.4 Japanese
```{r get_plots_japanese}
## Words
#### Age
###### Fuzzy
jpn.word.fuzzy.age.plot = rand.plotter(mod.words.fuzzy.age.rand.jpn, jpn.mod.w.f.a, "Age")

###### Strict
jpn.word.strict.age.plot = rand.plotter(mod.words.strict.age.rand.jpn, jpn.mod.w.s.a, "Age")

#### MLU
###### Fuzzy
jpn.word.fuzzy.mlu.plot = rand.plotter(mod.words.fuzzy.mlu.rand.jpn, jpn.mod.w.f.m, "MLU")

###### Strict
jpn.word.strict.mlu.plot = rand.plotter(mod.words.strict.mlu.rand.jpn, jpn.mod.w.s.m, "MLU")

#### PCA
###### Fuzzy
jpn.word.fuzzy.pca.plot = rand.plotter(mod.words.fuzzy.pca.rand.jpn, jpn.mod.w.f.p, "PCA")

###### Strict
jpn.word.strict.pca.plot = rand.plotter(mod.words.strict.pca.rand.jpn, jpn.mod.w.s.p, "PCA")

#### Entropy
###### Fuzzy
jpn.word.fuzzy.ent.plot = rand.plotter(mod.words.fuzzy.ent.rand.jpn, jpn.mod.w.f.h, "Lexical H")

###### Strict
jpn.word.strict.ent.plot = rand.plotter(mod.words.strict.ent.rand.jpn, jpn.mod.w.s.h, "Lexical H")

#### construct plot lists
jpn.word.fuzzy.list = list(jpn.word.fuzzy.age.plot,
                           jpn.word.fuzzy.mlu.plot,
                           jpn.word.fuzzy.ent.plot,
                           jpn.word.fuzzy.pca.plot)

jpn.word.strict.list = list(jpn.word.strict.age.plot,
                            jpn.word.strict.mlu.plot,
                            jpn.word.strict.ent.plot,
                            jpn.word.strict.pca.plot)
```
  
###### 4.1.9.2.5 Russian
```{r get_plots_russian}
## Words
#### Age
###### Fuzzy
rus.word.fuzzy.age.plot = rand.plotter(mod.words.fuzzy.age.rand.rus, rus.mod.w.f.a, "Age")

###### Strict
rus.word.strict.age.plot = rand.plotter(mod.words.strict.age.rand.rus, rus.mod.w.s.a, "Age")

#### MLU
###### Fuzzy
rus.word.fuzzy.mlu.plot = rand.plotter(mod.words.fuzzy.mlu.rand.rus, rus.mod.w.f.m, "MLU")

###### Strict
rus.word.strict.mlu.plot = rand.plotter(mod.words.strict.mlu.rand.rus, rus.mod.w.s.m, "MLU")

#### PCA
###### Fuzzy
rus.word.fuzzy.pca.plot = rand.plotter(mod.words.fuzzy.pca.rand.rus, rus.mod.w.f.p, "PCA")

###### Strict
rus.word.strict.pca.plot = rand.plotter(mod.words.strict.pca.rand.rus, rus.mod.w.s.p, "PCA")

#### Entropy
###### Fuzzy
rus.word.fuzzy.ent.plot = rand.plotter(mod.words.fuzzy.ent.rand.rus, rus.mod.w.f.h, "Lexical H")

###### Strict
rus.word.strict.ent.plot = rand.plotter(mod.words.strict.ent.rand.rus, rus.mod.w.s.h, "Lexical H")

#### construct plot lists
rus.word.fuzzy.list = list(rus.word.fuzzy.age.plot,
                           rus.word.fuzzy.mlu.plot,
                           rus.word.fuzzy.ent.plot,
                           rus.word.fuzzy.pca.plot)

rus.word.strict.list = list(rus.word.strict.age.plot,
                            rus.word.strict.mlu.plot,
                            rus.word.strict.ent.plot,
                            rus.word.strict.pca.plot)
```
  
###### 4.1.9.2.6 Sesotho
```{r get_plots_sesotho}
## Words
#### Age
###### Fuzzy
ses.word.fuzzy.age.plot = rand.plotter(mod.words.fuzzy.age.rand.ses, ses.mod.w.f.a, "Age")

###### Strict
ses.word.strict.age.plot = rand.plotter(mod.words.strict.age.rand.ses, ses.mod.w.s.a, "Age")

#### MLU
###### Fuzzy
ses.word.fuzzy.mlu.plot = rand.plotter(mod.words.fuzzy.mlu.rand.ses, ses.mod.w.f.m, "MLU")

###### Strict
ses.word.strict.mlu.plot = rand.plotter(mod.words.strict.mlu.rand.ses, ses.mod.w.s.m, "MLU")

#### PCA
###### Fuzzy
ses.word.fuzzy.pca.plot = rand.plotter(mod.words.fuzzy.pca.rand.ses, ses.mod.w.f.p, "PCA")

###### Strict
ses.word.strict.pca.plot = rand.plotter(mod.words.strict.pca.rand.ses, ses.mod.w.s.p, "PCA")

#### Entropy
###### Fuzzy
ses.word.fuzzy.ent.plot = rand.plotter(mod.words.fuzzy.ent.rand.ses, ses.mod.w.f.h, "Lexical H")

###### Strict
ses.word.strict.ent.plot = rand.plotter(mod.words.strict.ent.rand.ses, ses.mod.w.s.h, "Lexical H")

## Morphemes
#### Age
###### Fuzzy
ses.morph.fuzzy.age.plot = rand.plotter(mod.morph.fuzzy.age.rand.ses, ses.mod.m.f.a, "Age")

###### Strict
ses.morph.strict.age.plot = rand.plotter(mod.morph.strict.age.rand.ses, ses.mod.m.s.a, "Age")

#### MLU
###### Fuzzy
ses.morph.fuzzy.mlu.plot = rand.plotter(mod.morph.fuzzy.mlu.rand.ses, ses.mod.m.f.m, "MLU")

###### Strict
ses.morph.strict.mlu.plot = rand.plotter(mod.morph.strict.mlu.rand.ses, ses.mod.m.s.m, "MLU")

#### PCA
###### Fuzzy
ses.morph.fuzzy.pca.plot = rand.plotter(mod.morph.fuzzy.pca.rand.ses, ses.mod.m.f.p, "PCA")

###### Strict
ses.morph.strict.pca.plot = rand.plotter(mod.morph.strict.pca.rand.ses, ses.mod.m.s.p, "PCA")

#### Entropy
###### Fuzzy
ses.morph.fuzzy.ent.plot = rand.plotter(mod.morph.fuzzy.ent.rand.ses, ses.mod.m.f.h, "Morph. H")

###### Strict
ses.morph.strict.ent.plot = rand.plotter(mod.morph.strict.ent.rand.ses, ses.mod.m.s.h, "Morph. H")

#### construct plot lists
ses.word.fuzzy.list = list(ses.word.fuzzy.age.plot,
                           ses.word.fuzzy.mlu.plot,
                           ses.word.fuzzy.ent.plot,
                           ses.word.fuzzy.pca.plot)

ses.word.strict.list = list(ses.word.strict.age.plot,
                            ses.word.strict.mlu.plot,
                            ses.word.strict.ent.plot,
                            ses.word.strict.pca.plot)

ses.morph.fuzzy.list = list(ses.morph.fuzzy.age.plot,
                            ses.morph.fuzzy.mlu.plot,
                            ses.morph.fuzzy.ent.plot,
                            ses.morph.fuzzy.pca.plot)

ses.morph.strict.list = list(ses.morph.strict.age.plot,
                             ses.morph.strict.mlu.plot,
                             ses.morph.strict.ent.plot,
                             ses.morph.strict.pca.plot)
```
  
###### 4.1.9.2.7 Turkish
```{r get_plots_turkish}
## Words
#### Age
###### Fuzzy
tur.word.fuzzy.age.plot = rand.plotter(mod.words.fuzzy.age.rand.tur, tur.mod.w.f.a, "Age")

###### Strict
tur.word.strict.age.plot = rand.plotter(mod.words.strict.age.rand.tur, tur.mod.w.s.a, "Age")

#### MLU
###### Fuzzy
tur.word.fuzzy.mlu.plot = rand.plotter(mod.words.fuzzy.mlu.rand.tur, tur.mod.w.f.m, "MLU")

###### Strict
tur.word.strict.mlu.plot = rand.plotter(mod.words.strict.mlu.rand.tur, tur.mod.w.s.m, "MLU")

#### PCA
###### Fuzzy
tur.word.fuzzy.pca.plot = rand.plotter(mod.words.fuzzy.pca.rand.tur, tur.mod.w.f.p, "PCA")

###### Strict
tur.word.strict.pca.plot = rand.plotter(mod.words.strict.pca.rand.tur, tur.mod.w.s.p, "PCA")

#### Entropy
###### Fuzzy
tur.word.fuzzy.ent.plot = rand.plotter(mod.words.fuzzy.ent.rand.tur, tur.mod.w.f.h, "Lexical H")

###### Strict
tur.word.strict.ent.plot = rand.plotter(mod.words.strict.ent.rand.tur, tur.mod.w.s.h, "Lexical H")

## Morphemes
#### Age
###### Fuzzy
tur.morph.fuzzy.age.plot = rand.plotter(mod.morph.fuzzy.age.rand.tur, tur.mod.m.f.a, "Age")

###### Strict
tur.morph.strict.age.plot = rand.plotter(mod.morph.strict.age.rand.tur, tur.mod.m.s.a, "Age")

#### MLU
###### Fuzzy
tur.morph.fuzzy.mlu.plot = rand.plotter(mod.morph.fuzzy.mlu.rand.tur, tur.mod.m.f.m, "MLU")

###### Strict
tur.morph.strict.mlu.plot = rand.plotter(mod.morph.strict.mlu.rand.tur, tur.mod.m.s.m, "MLU")

#### PCA
###### Fuzzy
tur.morph.fuzzy.pca.plot = rand.plotter(mod.morph.fuzzy.pca.rand.tur, tur.mod.m.f.p, "PCA")

###### Strict
tur.morph.strict.pca.plot = rand.plotter(mod.morph.strict.pca.rand.tur, tur.mod.m.s.p, "PCA")

#### Entropy
###### Fuzzy
tur.morph.fuzzy.ent.plot = rand.plotter(mod.morph.fuzzy.ent.rand.tur, tur.mod.m.f.h, "Morph. H")

###### Strict
tur.morph.strict.ent.plot = rand.plotter(mod.morph.strict.ent.rand.tur, tur.mod.m.s.h, "Morph. H")

#### construct plot lists
tur.word.fuzzy.list = list(tur.word.fuzzy.age.plot,
                           tur.word.fuzzy.mlu.plot,
                           tur.word.fuzzy.ent.plot,
                           tur.word.fuzzy.pca.plot)

tur.word.strict.list = list(tur.word.strict.age.plot,
                            tur.word.strict.mlu.plot,
                            tur.word.strict.ent.plot,
                            tur.word.strict.pca.plot)

tur.morph.fuzzy.list = list(tur.morph.fuzzy.age.plot,
                            tur.morph.fuzzy.mlu.plot,
                            tur.morph.fuzzy.ent.plot,
                            tur.morph.fuzzy.pca.plot)

tur.morph.strict.list = list(tur.morph.strict.age.plot,
                             tur.morph.strict.mlu.plot,
                             tur.morph.strict.ent.plot,
                             tur.morph.strict.pca.plot)
```
  
###### 4.1.9.2.8 Yucatec
```{r get_plots_yucatec}
## Words
#### Age
###### Fuzzy
yuc.word.fuzzy.age.plot = rand.plotter(mod.words.fuzzy.age.rand.yuc, yuc.mod.w.f.a, "Age")

###### Strict
yuc.word.strict.age.plot = rand.plotter(mod.words.strict.age.rand.yuc, yuc.mod.w.s.a, "Age")

#### MLU
###### Fuzzy
yuc.word.fuzzy.mlu.plot = rand.plotter(mod.words.fuzzy.mlu.rand.yuc, yuc.mod.w.f.m, "MLU")

###### Strict
yuc.word.strict.mlu.plot = rand.plotter(mod.words.strict.mlu.rand.yuc, yuc.mod.w.s.m, "MLU")

#### PCA
###### Fuzzy
yuc.word.fuzzy.pca.plot = rand.plotter(mod.words.fuzzy.pca.rand.yuc, yuc.mod.w.f.p, "PCA")

###### Strict
yuc.word.strict.pca.plot = rand.plotter(mod.words.strict.pca.rand.yuc, yuc.mod.w.s.p, "PCA")

#### Entropy
###### Fuzzy
yuc.word.fuzzy.ent.plot = rand.plotter(mod.words.fuzzy.ent.rand.yuc, yuc.mod.w.f.h, "Lexical H")

###### Strict
yuc.word.strict.ent.plot = rand.plotter(mod.words.strict.ent.rand.yuc, yuc.mod.w.s.h, "Lexical H")

## Morphemes
#### Age
###### Fuzzy
yuc.morph.fuzzy.age.plot = rand.plotter(mod.morph.fuzzy.age.rand.yuc, yuc.mod.m.f.a, "Age")

###### Strict
yuc.morph.strict.age.plot = rand.plotter(mod.morph.strict.age.rand.yuc, yuc.mod.m.s.a, "Age")

#### MLU
###### Fuzzy
yuc.morph.fuzzy.mlu.plot = rand.plotter(mod.morph.fuzzy.mlu.rand.yuc, yuc.mod.m.f.m, "MLU")

###### Strict
yuc.morph.strict.mlu.plot = rand.plotter(mod.morph.strict.mlu.rand.yuc, yuc.mod.m.s.m, "MLU")

#### PCA
###### Fuzzy
yuc.morph.fuzzy.pca.plot = rand.plotter(mod.morph.fuzzy.pca.rand.yuc, yuc.mod.m.f.p, "PCA")

###### Strict
yuc.morph.strict.pca.plot = rand.plotter(mod.morph.strict.pca.rand.yuc, yuc.mod.m.s.p, "PCA")

#### Entropy
###### Fuzzy
yuc.morph.fuzzy.ent.plot = rand.plotter(mod.morph.fuzzy.ent.rand.yuc, yuc.mod.m.f.h, "Morph. H")

###### Strict
yuc.morph.strict.ent.plot = rand.plotter(mod.morph.strict.ent.rand.yuc, yuc.mod.m.s.h, "Morph. H")

#### construct plot lists
yuc.word.fuzzy.list = list(yuc.word.fuzzy.age.plot,
                           yuc.word.fuzzy.mlu.plot,
                           yuc.word.fuzzy.ent.plot,
                           yuc.word.fuzzy.pca.plot)

yuc.word.strict.list = list(yuc.word.strict.age.plot,
                            yuc.word.strict.mlu.plot,
                            yuc.word.strict.ent.plot,
                            yuc.word.strict.pca.plot)

yuc.morph.fuzzy.list = list(yuc.morph.fuzzy.age.plot,
                            yuc.morph.fuzzy.mlu.plot,
                            yuc.morph.fuzzy.ent.plot,
                            yuc.morph.fuzzy.pca.plot)

yuc.morph.strict.list = list(yuc.morph.strict.age.plot,
                             yuc.morph.strict.mlu.plot,
                             yuc.morph.strict.ent.plot,
                             yuc.morph.strict.pca.plot)
```
  
##### 4.1.9.3 Collating plots
Finally, we gather up the rows and plot, one grid each for fuzzy + word, fuzzy + morpheme, strict + word, and strict + morpheme. 
```{r gather_rows_and_plot, fig.height=10}
####################
# Construct the rows
####################
# Chintang
## Words
#### Fuzzy
cnt.word.fuzzy.row = rand.row.collector(cnt.word.fuzzy.list,
                                        label = "Chintang ")

#### Strict
cnt.word.strict.row = rand.row.collector(cnt.word.strict.list,
                                         label = "Chintang ")
## Morphemes
#### Fuzzy
cnt.morph.fuzzy.row = rand.row.collector(cnt.morph.fuzzy.list,
                                        label = "Chintang ")

#### Strict
cnt.morph.strict.row = rand.row.collector(cnt.morph.strict.list,
                                         label = "Chintang ")

# English
## Words
#### Fuzzy
eng.word.fuzzy.row = rand.row.collector(eng.word.fuzzy.list,
                                        label = "English  ")

#### Strict
eng.word.strict.row = rand.row.collector(eng.word.strict.list,
                                         label = "English  ")

# Inuktitut
## Words
#### Fuzzy
ink.word.fuzzy.row = rand.row.collector(ink.word.fuzzy.list,
                                        label = "Inuktitut")

#### Strict
ink.word.strict.row = rand.row.collector(ink.word.strict.list,
                                         label = "Inuktitut")
## Morphemes
#### Fuzzy
ink.morph.fuzzy.row = rand.row.collector(ink.morph.fuzzy.list,
                                        label = "Inuktitut")

#### Strict
ink.morph.strict.row = rand.row.collector(ink.morph.strict.list,
                                         label = "Inuktitut")

# Japanese
## Words
#### Fuzzy
jpn.word.fuzzy.row = rand.row.collector(jpn.word.fuzzy.list,
                                        label = "Japanese ")

#### Strict
jpn.word.strict.row = rand.row.collector(jpn.word.strict.list,
                                         label = "Japanese ")

# Russian
# English
## Words
#### Fuzzy
rus.word.fuzzy.row = rand.row.collector(rus.word.fuzzy.list,
                                        label = "Russian  ")

#### Strict
rus.word.strict.row = rand.row.collector(rus.word.strict.list,
                                         label = "Russian  ")

# Sesotho
## Words
#### Fuzzy
ses.word.fuzzy.row = rand.row.collector(ses.word.fuzzy.list,
                                        label = "Sesotho  ")

#### Strict
ses.word.strict.row = rand.row.collector(ses.word.strict.list,
                                         label = "Sesotho  ")
## Morphemes
#### Fuzzy
ses.morph.fuzzy.row = rand.row.collector(ses.morph.fuzzy.list,
                                        label = "Sesotho  ")

#### Strict
ses.morph.strict.row = rand.row.collector(ses.morph.strict.list,
                                         label = "Sesotho  ")

# Turkish
## Words
#### Fuzzy
tur.word.fuzzy.row = rand.row.collector(tur.word.fuzzy.list,
                                        label = "Turkish  ")

#### Strict
tur.word.strict.row = rand.row.collector(tur.word.strict.list,
                                         label = "Turkish  ")
## Morphemes
#### Fuzzy
tur.morph.fuzzy.row = rand.row.collector(tur.morph.fuzzy.list,
                                        label = "Turkish  ")

#### Strict
tur.morph.strict.row = rand.row.collector(tur.morph.strict.list,
                                         label = "Turkish  ")

# Yucatec
## Words
#### Fuzzy
yuc.word.fuzzy.row = rand.row.collector(yuc.word.fuzzy.list,
                                        label = "Yucatec  ")

#### Strict
yuc.word.strict.row = rand.row.collector(yuc.word.strict.list,
                                         label = "Yucatec  ")
## Morphemes
#### Fuzzy
yuc.morph.fuzzy.row = rand.row.collector(yuc.morph.fuzzy.list,
                                        label = "Yucatec  ")

#### Strict
yuc.morph.strict.row = rand.row.collector(yuc.morph.strict.list,
                                         label = "Yucatec  ")

##################
# Combine the rows
##################
# Words
## Fuzzy
all.word.fuzzy.rand.plot = ggarrange(cnt.word.fuzzy.row,
                                     eng.word.fuzzy.row,
                                     ink.word.fuzzy.row,
                                     jpn.word.fuzzy.row,
                                     rus.word.fuzzy.row,
                                     ses.word.fuzzy.row,
                                     tur.word.fuzzy.row,
                                     yuc.word.fuzzy.row,
                                     nrow=8,
                                     align="v")

## Strict
all.word.strict.rand.plot = ggarrange(cnt.word.strict.row,
                                      eng.word.strict.row,
                                      ink.word.strict.row,
                                      jpn.word.strict.row,
                                      rus.word.strict.row,
                                      ses.word.strict.row,
                                      tur.word.strict.row,
                                      yuc.word.strict.row,
                                      nrow=8,
                                      align="v")

# Morphemes
## Fuzzy
all.morph.fuzzy.rand.plot = ggarrange(cnt.morph.fuzzy.row,
                                     ink.morph.fuzzy.row,
                                     ses.morph.fuzzy.row,
                                     tur.morph.fuzzy.row,
                                     yuc.morph.fuzzy.row,
                                     nrow=5,
                                     align="v")

## Strict
all.morph.strict.rand.plot = ggarrange(cnt.morph.strict.row,
                                      ink.morph.strict.row,
                                      ses.morph.strict.row,
                                      tur.morph.strict.row,
                                      yuc.morph.strict.row,
                                      nrow=5,
                                      align="hv") 

# Get the legend
legend = get_legend(
         rand.plotter(mod.words.fuzzy.age.rand.tur, tur.mod.w.f.a, "Age", T) +
         theme( legend.margin = margin(0, 0, 0, 0),
                legend.position = "bottom",
                legend.key.size = unit(1, 'cm'), #change legend key size
                legend.key.height = unit(1, 'cm'), #change legend key height
                legend.key.width = unit(1, 'cm'), #change legend key width
                legend.title = element_text(size=14), #change legend title font size
                legend.text = element_text(size=10)) #change legend text font size 
)

# Add the legend
all.word.fuzzy.rand.plot = plot_grid(all.word.fuzzy.rand.plot, legend, rel_heights = c(0.9, 0.1), nrow=2); all.word.fuzzy.rand.plot

all.word.strict.rand.plot = plot_grid(all.word.strict.rand.plot, legend, rel_heights = c(0.9, 0.1), nrow=2); all.word.strict.rand.plot

all.morph.fuzzy.rand.plot = plot_grid(all.morph.fuzzy.rand.plot, legend, rel_heights = c(0.9, 0.1), nrow=2); all.morph.fuzzy.rand.plot

all.morph.strict.rand.plot = plot_grid(all.morph.strict.rand.plot, legend, rel_heights = c(0.9, 0.1), nrow=2); all.morph.strict.rand.plot
```

##### 4.1.9.4 Save the plots (for the manuscript)
```{r save_plots_for_ms}
ggsave("../Results/Plots/word_fuzzy_css_rand.tiff", all.word.fuzzy.rand.plot, dpi = "print", width=7.3, height = 12, units="in", device="tiff")

ggsave("../Results/Plots/word_strict_css_rand.tiff", all.word.strict.rand.plot, dpi = "print", width=7.3, height = 12, units="in", device="tiff")

ggsave("../Results/Plots/morph_fuzzy_css_rand.tiff", all.morph.fuzzy.rand.plot, dpi = "print", width=7.3, height = 7.5, units="in", device="tiff")

ggsave("../Results/Plots/morph_strict_css_rand.tiff", all.morph.strict.rand.plot, dpi = "print", width=7.3, height = 7.5, units="in", device="tiff")
```

### 4.2 Child-surrounding speech vs. adult-directed speech
Here we compare adults to children in based on developmental groups. We take this step because, while the CSS data are longitudinal, the adult data are not. As with the primary analysis of CSS, we compare groupings based on multiple indices of development: age, MLU, lexical/morphological entropy, and PCA. Adults are always assigned to the same category. Other groups are assigned based on a four-way split into equal-sized groups across all corpora.  
  
#### 4.2.1 Chintang
**Age**
```{r rand_baseline_analysis_ads_v_css_chintang_age}
#### Fuzzy
mod.age.fuzzy.rand.cnt = lmer(logit.vs.percentage ~ random*age.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.cnt.rand %>% filter(SD.outliers=="no"))

anova(mod.age.fuzzy.rand.cnt)
summary(mod.age.fuzzy.rand.cnt)

#### Strict
mod.age.strict.rand.cnt = lmer(logit.vs.percentage ~ random*age.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.cnt.rand %>% filter(SD.outliers=="no"))

anova(mod.age.strict.rand.cnt)
summary(mod.age.strict.rand.cnt)
```
  
**MLU**
```{r rand_baseline_analysis_ads_v_css_chintang_mlu}
#### Fuzzy
mod.mlu.fuzzy.rand.cnt = lmer(logit.vs.percentage ~ random*MLUw.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.cnt.rand %>% filter(SD.outliers=="no"))

anova(mod.mlu.fuzzy.rand.cnt)
summary(mod.mlu.fuzzy.rand.cnt)

#### Strict
mod.mlu.strict.rand.cnt = lmer(logit.vs.percentage ~ random*MLUw.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.cnt.rand %>% filter(SD.outliers=="no"))

anova(mod.mlu.strict.rand.cnt)
summary(mod.mlu.strict.rand.cnt)
```
  
**Entropy**
```{r rand_baseline_analysis_ads_v_css_chintang_H}
#### Fuzzy
mod.ent.fuzzy.rand.cnt = lmer(logit.vs.percentage ~ random*WordEnt.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.cnt.rand %>% filter(SD.outliers=="no"))

anova(mod.ent.fuzzy.rand.cnt)
summary(mod.ent.fuzzy.rand.cnt)

#### Strict
mod.ent.strict.rand.cnt = lmer(logit.vs.percentage ~ random*WordEnt.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.cnt.rand %>% filter(SD.outliers=="no"))

anova(mod.ent.strict.rand.cnt)
summary(mod.ent.strict.rand.cnt)
```
  
**PCA**
```{r rand_baseline_analysis_ads_v_css_chintang_pca}
#### Fuzzy
mod.pca.fuzzy.rand.cnt = lmer(logit.vs.percentage ~ random*PC.word.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.cnt.rand %>% filter(SD.outliers=="no"))

anova(mod.pca.fuzzy.rand.cnt)
summary(mod.pca.fuzzy.rand.cnt)

#### Strict
mod.pca.strict.rand.cnt = lmer(logit.vs.percentage ~ random*PC.word.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.cnt.rand %>% filter(SD.outliers=="no"))

anova(mod.pca.strict.rand.cnt)
summary(mod.pca.strict.rand.cnt)
```

#### 4.2.2 English
**Age**
```{r rand_baseline_analysis_ads_v_css_english_age}
#### Fuzzy
mod.age.fuzzy.rand.eng = lmer(logit.vs.percentage ~ random*age.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.eng.rand %>% filter(SD.outliers=="no"))

anova(mod.age.fuzzy.rand.eng)
summary(mod.age.fuzzy.rand.eng)

#### Strict
mod.age.strict.rand.eng = lmer(logit.vs.percentage ~ random*age.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.eng.rand %>% filter(SD.outliers=="no"))

anova(mod.age.strict.rand.eng)
summary(mod.age.strict.rand.eng)
```
  
**MLU**
```{r rand_baseline_analysis_ads_v_css_english_mlu}
#### Fuzzy
mod.mlu.fuzzy.rand.eng = lmer(logit.vs.percentage ~ random*MLUw.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.eng.rand %>% filter(SD.outliers=="no"))

anova(mod.mlu.fuzzy.rand.eng)
summary(mod.mlu.fuzzy.rand.eng)

#### Strict
mod.mlu.strict.rand.eng = lmer(logit.vs.percentage ~ random*MLUw.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.eng.rand %>% filter(SD.outliers=="no"))

anova(mod.mlu.strict.rand.eng)
summary(mod.mlu.strict.rand.eng)
```
  
**Entropy**
```{r rand_baseline_analysis_ads_v_css_english_H}
#### Fuzzy
mod.ent.fuzzy.rand.eng = lmer(logit.vs.percentage ~ random*WordEnt.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.eng.rand %>% filter(SD.outliers=="no"))

anova(mod.ent.fuzzy.rand.eng)
summary(mod.ent.fuzzy.rand.eng)

#### Strict
mod.ent.strict.rand.eng = lmer(logit.vs.percentage ~ random*WordEnt.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.eng.rand %>% filter(SD.outliers=="no"))

anova(mod.ent.strict.rand.eng)
summary(mod.ent.strict.rand.eng)
```
  
**PCA**
```{r rand_baseline_analysis_ads_v_css_english_pca}
#### Fuzzy
mod.pca.fuzzy.rand.eng = lmer(logit.vs.percentage ~ random*PC.word.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.fuzzy.eng.rand %>% filter(SD.outliers=="no"))

anova(mod.pca.fuzzy.rand.eng)
summary(mod.pca.fuzzy.rand.eng)

#### Strict
mod.pca.strict.rand.eng = lmer(logit.vs.percentage ~ random*PC.word.groups + window_size + mean.nv.lens.words.scaled + number.of.utterances.scaled + number.of.speakers.scaled + (1|session_id), data = ads.strict.eng.rand %>% filter(SD.outliers=="no"))

anova(mod.pca.fuzzy.rand.eng)
summary(mod.pca.fuzzy.rand.eng)
```

#### 4.2.3 Plotting
**Function for plotting**
```{r plotting_ads_results_function}
ads.rand.plotter = function(mod, title){
    eff.obj = allEffects(mod)[5][[1]]
    anova.obj = anova(mod)
    p.val = anova.obj[[6]][7]
    stars.int = p.finder(p.val)
    eff.matrix = as.data.frame(eff.obj$model.matrix)
    ngroups = nrow(eff.matrix)/2
    eff.fit = bind_cols(eff.matrix,
                        fit = ilogit(eff.obj$fit),
                        lower = ilogit(eff.obj$lower),
                        upper = ilogit(eff.obj$upper)) %>%
                        rename(random = randomrandom) %>%
                        mutate(random = ifelse(random==0, "original", "random"),
                               group = rep(c(seq(1, ngroups-1), "adult"), each=2))
    p = ggplot(eff.fit, aes(x = group, y = fit, color=random)) +
               geom_point(size=.5) +
               geom_errorbar(aes(ymin=lower, ymax=upper), width=.2, fatten=2) +
               theme_bw() +
               ylim(0,1) +
               labs(color = "Text type") +
               ylab(element_blank()) +
               xlab(element_blank()) +
               ggtitle(paste0(title, stars.int)) +
               theme(legend.position = "none",
                     text = element_text(size = 14, family="Times New Roman"),
                     plot.title = element_text(hjust=0.5))
    
    p = p + scale_color_manual(values = c("dodgerblue", "darkred"))
    
    return(p)
}
```

**Create plots**
```{r plotting_ads_random_analysis}
# Chintang
## Age groups
#### Fuzzy
cnt.agegrp.fuzzy.plot = ads.rand.plotter(mod.age.fuzzy.rand.cnt, "Age")

#### Strict
cnt.agegrp.strict.plot = ads.rand.plotter(mod.age.strict.rand.cnt, "Age")

## MLU groups
#### Fuzzy
cnt.mlugrp.fuzzy.plot = ads.rand.plotter(mod.mlu.fuzzy.rand.cnt, "MLU")

#### Strict
cnt.mlugrp.strict.plot = ads.rand.plotter(mod.mlu.strict.rand.cnt, "MLU")

## Entropy groups
#### Fuzzy
cnt.entgrp.fuzzy.plot = ads.rand.plotter(mod.ent.fuzzy.rand.cnt, "Lexical H")

#### Strict
cnt.entgrp.strict.plot = ads.rand.plotter(mod.ent.strict.rand.cnt, "Lexical H")

## PCA groups
#### Fuzzy
cnt.pcagrp.fuzzy.plot = ads.rand.plotter(mod.pca.fuzzy.rand.cnt, "PCA")

#### Strict
cnt.pcagrp.strict.plot = ads.rand.plotter(mod.pca.strict.rand.cnt, "PCA")

# English
## Age groups
#### Fuzzy
eng.agegrp.fuzzy.plot = ads.rand.plotter(mod.age.fuzzy.rand.eng, "Age")

#### Strict
eng.agegrp.strict.plot = ads.rand.plotter(mod.age.strict.rand.eng, "Age")

## MLU groups
#### Fuzzy
eng.mlugrp.fuzzy.plot = ads.rand.plotter(mod.mlu.fuzzy.rand.eng, "MLU")

#### Strict
eng.mlugrp.strict.plot = ads.rand.plotter(mod.mlu.strict.rand.eng, "MLU")

## Entropy groups
#### Fuzzy
eng.entgrp.fuzzy.plot = ads.rand.plotter(mod.ent.fuzzy.rand.eng, "Lexical H")

#### Strict
eng.entgrp.strict.plot = ads.rand.plotter(mod.ent.strict.rand.eng, "Lexical H")

## PCA groups
#### Fuzzy
eng.pcagrp.fuzzy.plot = ads.rand.plotter(mod.pca.fuzzy.rand.eng, "PCA")

#### Strict
eng.pcagrp.strict.plot = ads.rand.plotter(mod.pca.strict.rand.eng, "PCA")

# Collect fuzzy plot rows
fuzzy.ads.eng.list = list(eng.agegrp.fuzzy.plot, 
                          eng.mlugrp.fuzzy.plot, 
                          eng.entgrp.fuzzy.plot,
                          eng.pcagrp.fuzzy.plot)

fuzzy.ads.eng.row = rand.row.collector(fuzzy.ads.eng.list, "English ")

fuzzy.ads.cnt.list = list(cnt.agegrp.fuzzy.plot,
                          cnt.mlugrp.fuzzy.plot,
                          cnt.entgrp.fuzzy.plot,
                          cnt.pcagrp.fuzzy.plot)

fuzzy.ads.cnt.row = rand.row.collector(fuzzy.ads.cnt.list, "Chintang")

all.fuzzy.ads.rand.plot = ggarrange(fuzzy.ads.cnt.row,
                                    fuzzy.ads.eng.row,
                                    nrow=2)

all.fuzzy.ads.rand.plot = plot_grid(all.fuzzy.ads.rand.plot, legend, rel_heights = c(0.9, 0.1), nrow=2); all.fuzzy.ads.rand.plot

# Collect strict plot rows
strict.ads.eng.list = list(eng.agegrp.strict.plot, 
                           eng.mlugrp.strict.plot, 
                           eng.entgrp.strict.plot,
                           eng.pcagrp.strict.plot)

strict.ads.eng.row = rand.row.collector(strict.ads.eng.list, "English ")


strict.ads.cnt.list = list(cnt.agegrp.strict.plot,
                           cnt.mlugrp.strict.plot,
                           cnt.entgrp.strict.plot,
                           cnt.pcagrp.strict.plot)

strict.ads.cnt.row = rand.row.collector(strict.ads.cnt.list, "Chintang")

all.strict.ads.rand.plot = ggarrange(strict.ads.cnt.row,
                                     strict.ads.eng.row,
                                     nrow=2)

all.strict.ads.rand.plot = plot_grid(all.strict.ads.rand.plot, legend, rel_heights = c(0.9, 0.1), nrow=2); all.strict.ads.rand.plot
```

**Save the plots**
```{r save_ads_css_rand_plots}
ggsave("../Results/Plots/word_fuzzy_ads_rand.tiff", all.fuzzy.ads.rand.plot, dpi = "print", width=7.6, height = 4, units="in", device="tiff")

ggsave("../Results/Plots/word_strict_ads_rand.tiff", all.strict.ads.rand.plot, dpi = "print", width=7.6, height = 4, units="in", device="tiff")
```